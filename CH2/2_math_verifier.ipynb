{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "15ef20b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import torch\n",
    "from reasoning_from_scratch.ch02 import (\n",
    "        get_device\n",
    ")\n",
    "from reasoning_from_scratch.qwen3 import (\n",
    "        download_qwen3_small,\n",
    "        Qwen3Tokenizer,\n",
    "        Qwen3Model,\n",
    "        QWEN_CONFIG_06_B\n",
    ")\n",
    "from reasoning_from_scratch.qwen3 import KVCache\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "07b39c3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model_and_tokenizer(which_model, device, use_compile, local_dir=\"qwen3\"):\n",
    "\n",
    "    if which_model == \"base\":\n",
    "        download_qwen3_small(kind=\"base\", tokenizer_only=False, out_dir=local_dir)\n",
    "        tokenizer_path = Path(local_dir) / \"tokenizer-base.json\"\n",
    "        model_path = Path(local_dir) / \"qwen3-0.6B-base.pth\"\n",
    "        tokenizer = Qwen3Tokenizer(tokenizer_file_path=tokenizer_path)\n",
    "\n",
    "    elif which_model == \"reasoning\":\n",
    "        download_qwen3_small(kind=\"reasoning\", tokenizer_only=False, out_dir=local_dir)\n",
    "        tokenizer_path = Path(local_dir) / \"tokenizer-reasoning.json\"\n",
    "        model_path = Path(local_dir) / \"qwen3-0.6B-reasoning.pth\"\n",
    "        tokenizer = Qwen3Tokenizer(\n",
    "                tokenizer_file_path=tokenizer_path,\n",
    "                apply_chat_template=True,\n",
    "                add_generation_prompt=True,\n",
    "                add_thinking=True,\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(f\"Invalid choice: which_model={which_model}\")\n",
    "    \n",
    "    model = Qwen3Model(QWEN_CONFIG_06_B)\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "    model.to(device)\n",
    "    if use_compile: #Optionally set to true to enable model compilation\n",
    "        torch._dynamo.config.allow_unspec_int_on_nn_module = True\n",
    "        model = torch.compile(model)\n",
    "    return model, tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b0aae7af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using NVIDIA CUDA GPU\n",
      "✓ qwen3\\qwen3-0.6B-base.pth already up-to-date\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Utente\\AppData\\Local\\Temp\\ipykernel_395256\\2595948078.py:23: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(model_path))\n"
     ]
    }
   ],
   "source": [
    "WHICH_MODEL = \"base\" #Uses the base model, similar to chapter 2, by default\n",
    "device = get_device()\n",
    "model, tokenizer = load_model_and_tokenizer(\n",
    "        which_model=WHICH_MODEL,\n",
    "        device=device,\n",
    "        use_compile=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "31a7bde6",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.inference_mode()\n",
    "def generate_text_basic_stream_cache(model, input_ids, max_new_tokens, eos_token_id=None):\n",
    "    model.eval()\n",
    "\n",
    "    cache = KVCache(n_layers=model.cfg['n_layers'])\n",
    "    model.reset_kv_cache()\n",
    "\n",
    "    out = model(input_ids, cache=cache)[:, -1]\n",
    "\n",
    "    for _ in range(max_new_tokens):\n",
    "        next_token = torch.argmax(out, dim=-1, keepdim=True)\n",
    "\n",
    "        if (eos_token_id is not None\n",
    "                and next_token.item() == eos_token_id):\n",
    "            break\n",
    "\n",
    "        yield next_token  # Yield each token as it's generated\n",
    "\n",
    "        out = model(next_token, cache=cache)[:, -1]\n",
    "        \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "faf69eca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " To find the value of \\( a^2 + b^2 \\) given that \\( a + b = 3 \\) and \\( ab = \\frac{13}{6} \\), we can use the following algebraic identity:\n",
      "\n",
      "\\[\n",
      "a^2 + b^2 = (a + b)^2 - 2ab\n",
      "\\]\n",
      "\n",
      "**Step 1:** Substitute the given values into the equation.\n",
      "\n",
      "\\[\n",
      "a^2 + b^2 = (3)^2 - 2 \\left( \\frac{13}{6} \\right)\n",
      "\\]\n",
      "\n",
      "**Step 2:** Calculate \\( (3)^2 \\).\n",
      "\n",
      "\\[\n",
      "(3)^2 = 9\n",
      "\\]\n",
      "\n",
      "**Step 3:** Calculate \\( 2 \\times \\frac{13}{6} \\).\n",
      "\n",
      "\\[\n",
      "2 \\times \\frac{13}{6} = \\frac{26}{6} = \\frac{13}{3}\n",
      "\\]\n",
      "\n",
      "**Step 4:** Subtract the second result from the first.\n",
      "\n",
      "\\[\n",
      "a^2 + b^2 = 9 - \\frac{13}{3}\n",
      "\\]\n",
      "\n",
      "**Step 5:** Convert 9 to a fraction with a denominator of 3 to perform the subtraction.\n",
      "\n",
      "\\[\n",
      "9 = \\frac{27}{3}\n",
      "\\]\n",
      "\n",
      "\\[\n",
      "a^2 + b^2 = \\frac{27}{3} - \\frac{13}{3} = \\frac{14}{3}\n",
      "\\]\n",
      "\n",
      "**Final Answer:**\n",
      "\n",
      "\\[\n",
      "\\boxed{\\dfrac{14}{3}}\n",
      "\\]"
     ]
    }
   ],
   "source": [
    "prompt = ( #MATH PROBLEM\n",
    "    r\"If $a+b=3$ and $ab=\\tfrac{13}{6}$, \"\n",
    "    r\"what is the value of $a^2+b^2$?\"\n",
    ")\n",
    "\n",
    "input_ids = torch.tensor(\n",
    "    tokenizer.encode(prompt),\n",
    "    device=device\n",
    ").unsqueeze(0)\n",
    "\n",
    "all_token_ids = []\n",
    "\n",
    "for token in generate_text_basic_stream_cache(model, input_ids, max_new_tokens=2048, eos_token_id=tokenizer.eos_token_id):\n",
    "    token_id = token.squeeze(0)\n",
    "    decoded_id = tokenizer.decode([token_id])\n",
    "    print(\n",
    "        decoded_id,\n",
    "        end='',\n",
    "        flush=True\n",
    "    )\n",
    "\n",
    "    all_token_ids.append(token_id)\n",
    "\n",
    "all_tokens = tokenizer.decode(all_token_ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7a3f21db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       " To find the value of \\( a^2 + b^2 \\) given that \\( a + b = 3 \\) and \\( ab = \\frac{13}{6} \\), we can use the following algebraic identity:\n",
       "\n",
       "\\[\n",
       "a^2 + b^2 = (a + b)^2 - 2ab\n",
       "\\]\n",
       "\n",
       "**Step 1:** Substitute the given values into the equation.\n",
       "\n",
       "\\[\n",
       "a^2 + b^2 = (3)^2 - 2 \\left( \\frac{13}{6} \\right)\n",
       "\\]\n",
       "\n",
       "**Step 2:** Calculate \\( (3)^2 \\).\n",
       "\n",
       "\\[\n",
       "(3)^2 = 9\n",
       "\\]\n",
       "\n",
       "**Step 3:** Calculate \\( 2 \\times \\frac{13}{6} \\).\n",
       "\n",
       "\\[\n",
       "2 \\times \\frac{13}{6} = \\frac{26}{6} = \\frac{13}{3}\n",
       "\\]\n",
       "\n",
       "**Step 4:** Subtract the second result from the first.\n",
       "\n",
       "\\[\n",
       "a^2 + b^2 = 9 - \\frac{13}{3}\n",
       "\\]\n",
       "\n",
       "**Step 5:** Convert 9 to a fraction with a denominator of 3 to perform the subtraction.\n",
       "\n",
       "\\[\n",
       "9 = \\frac{27}{3}\n",
       "\\]\n",
       "\n",
       "\\[\n",
       "a^2 + b^2 = \\frac{27}{3} - \\frac{13}{3} = \\frac{14}{3}\n",
       "\\]\n",
       "\n",
       "**Final Answer:**\n",
       "\n",
       "\\[\n",
       "\\boxed{\\dfrac{14}{3}}\n",
       "\\]"
      ],
      "text/plain": [
       "<IPython.core.display.Latex object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Latex, display\n",
    "display(Latex(all_tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a47ee8a1",
   "metadata": {},
   "source": [
    "Now we can prepare the wrapper function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0dffb96f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text_stream_concat(model, tokenizer, prompt, device, max_new_tokens, verbose=False):\n",
    "\n",
    "    input_ids = torch.tensor(\n",
    "        tokenizer.encode(prompt),\n",
    "        device=device\n",
    "    ).unsqueeze(0)\n",
    "\n",
    "    all_token_ids = []\n",
    "\n",
    "    for token in generate_text_basic_stream_cache(model, input_ids, max_new_tokens=max_new_tokens, eos_token_id=tokenizer.eos_token_id):\n",
    "        token_id = token.squeeze(0)\n",
    "        all_token_ids.append(token_id)\n",
    "        \n",
    "\n",
    "        if verbose:\n",
    "            decoded_id = tokenizer.decode([token_id])\n",
    "            print(\n",
    "                decoded_id,\n",
    "                end='',\n",
    "                flush=True\n",
    "            )\n",
    "\n",
    "    return tokenizer.decode(all_token_ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a92341b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " To find the value of \\( a^2 + b^2 \\) given that \\( a + b = 3 \\) and \\( ab = \\frac{13}{6} \\), we can use the following algebraic identity:\n",
      "\n",
      "\\[\n",
      "a^2 + b^2 = (a + b)^2 - 2ab\n",
      "\\]\n",
      "\n",
      "**Step 1:** Substitute the given values into the equation.\n",
      "\n",
      "\\[\n",
      "a^2 + b^2 = (3)^2 - 2 \\left( \\frac{13}{6} \\right)\n",
      "\\]\n",
      "\n",
      "**Step 2:** Calculate \\( (3)^2 \\).\n",
      "\n",
      "\\[\n",
      "(3)^2 = 9\n",
      "\\]\n",
      "\n",
      "**Step 3:** Calculate \\( 2 \\times \\frac{13}{6} \\).\n",
      "\n",
      "\\[\n",
      "2 \\times \\frac{13}{6} = \\frac{26}{6} = \\frac{13}{3}\n",
      "\\]\n",
      "\n",
      "**Step 4:** Subtract the second result from the first.\n",
      "\n",
      "\\[\n",
      "a^2 + b^2 = 9 - \\frac{13}{3}\n",
      "\\]\n",
      "\n",
      "**Step 5:** Convert 9 to a fraction with a denominator of 3 to perform the subtraction.\n",
      "\n",
      "\\[\n",
      "9 = \\frac{27}{3}\n",
      "\\]\n",
      "\n",
      "\\[\n",
      "a^2 + b^2 = \\frac{27}{3} - \\frac{13}{3} = \\frac{14}{3}\n",
      "\\]\n",
      "\n",
      "**Final Answer:**\n",
      "\n",
      "\\[\n",
      "\\boxed{\\dfrac{14}{3}}\n",
      "\\]"
     ]
    }
   ],
   "source": [
    "model_answer = generate_text_stream_concat(model, tokenizer, prompt, device, max_new_tokens=2048, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "677de1e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_last_boxed(text):\n",
    "    boxed_start_idx = text.rfind(r\"\\boxed\") \n",
    "    if boxed_start_idx == -1:\n",
    "        return None\n",
    "    current_idx = boxed_start_idx + len(r\"\\boxed\") \n",
    "\n",
    "    while current_idx < len(text) and text[current_idx].isspace():\n",
    "        current_idx += 1\n",
    "    \n",
    "    if current_idx >= len(text) or text[current_idx] != \"{\":\n",
    "        return None\n",
    "    \n",
    "    current_idx += 1\n",
    "    brace_depth = 1\n",
    "    content_start_idx = current_idx\n",
    "    \n",
    "    while current_idx < len(text) and brace_depth > 0:\n",
    "        char = text[current_idx]\n",
    "        if char == \"{\":\n",
    "            brace_depth += 1\n",
    "        elif char == \"}\":\n",
    "            brace_depth -= 1\n",
    "        current_idx += 1\n",
    "\n",
    "    if brace_depth != 0: \n",
    "        return None\n",
    "    \n",
    "    return text[content_start_idx:current_idx-1] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4c11632b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle \\dfrac{14}{3}$"
      ],
      "text/plain": [
       "<IPython.core.display.Math object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Math\n",
    "\n",
    "extracted_answer = get_last_boxed(model_answer)\n",
    "display(Math(extracted_answer))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0346137e",
   "metadata": {},
   "source": [
    "We can also handle cases in which the model fails to format a correct answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9ffec0de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "RE_NUMBER = re.compile( \n",
    "        r\"-?(?:\\d+/\\d+|\\d+(?:\\.\\d+)?(?:[eE][+-]?\\d+)?)\"\n",
    "    )\n",
    "def extract_final_candidate(text, fallback=\"number_then_full\"):\n",
    "    result = \"\" \n",
    "\n",
    "    if text: \n",
    "        boxed = get_last_boxed(text.strip())\n",
    "\n",
    "        if boxed:\n",
    "            result = boxed.strip().strip(\"$ \")\n",
    "    \n",
    "        elif fallback in (\"number_then_full\", \"number_only\"):\n",
    "            m = RE_NUMBER.findall(text)\n",
    "\n",
    "            if m:\n",
    "                result = m[-1] \n",
    "            elif fallback == \"number_then_full\":\n",
    "                result = text \n",
    "\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5e0962d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle 14/3.$"
      ],
      "text/plain": [
       "<IPython.core.display.Math object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "final_candidate = extract_final_candidate(r\"\\boxed{ 14/3. }\")\n",
    "display(Math(final_candidate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7c37a411",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle 14/3$"
      ],
      "text/plain": [
       "<IPython.core.display.Math object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "final_candidate = extract_final_candidate(\"abc < > 14/3 abc\") #last number is 14/3\n",
    "display(Math(final_candidate))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c1eec20",
   "metadata": {},
   "source": [
    "Now we can NORMALIZE the extracted answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e372e9d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "LATEX_FIXES = [  # A\n",
    "    (r\"\\\\left\\s*\", \"\"),\n",
    "    (r\"\\\\right\\s*\", \"\"),\n",
    "    (r\"\\\\,|\\\\!|\\\\;|\\\\:\", \"\"),\n",
    "    (r\"\\\\cdot\", \"*\"),\n",
    "    (r\"\\u00B7|\\u00D7\", \"*\"),\n",
    "    (r\"\\\\\\^\\\\circ\", \"\"),\n",
    "    (r\"\\\\dfrac\", r\"\\\\frac\"),\n",
    "    (r\"\\\\tfrac\", r\"\\\\frac\"),\n",
    "    (r\"°\", \"\"),\n",
    "]\n",
    "RE_SPECIAL = re.compile(r\"<\\|[^>]+?\\|>\")  # B\n",
    "SUPERSCRIPT_MAP = {\n",
    "    \"⁰\": \"0\", \"¹\": \"1\", \"²\": \"2\", \"³\": \"3\", \"⁴\": \"4\",  # C\n",
    "    \"⁵\": \"5\", \"⁶\": \"6\", \"⁷\": \"7\", \"⁸\": \"8\", \"⁹\": \"9\",  # C\n",
    "    \"⁺\": \"+\", \"⁻\": \"-\", \"⁽\": \"(\", \"⁾\": \")\",  # C\n",
    "}\n",
    "\n",
    "def normalize_text(text):\n",
    "    if not text:\n",
    "        return \"\"\n",
    "    text = RE_SPECIAL.sub(\"\", text).strip()\n",
    "    # D\n",
    "    match = re.match(r\"^[A-Za-z]\\s*[.:]\\s*(.+)$\", text)\n",
    "    if match:\n",
    "        text = match.group(1)\n",
    "    text = re.sub(r\"\\^\\s*\\{\\s*\\\\circ\\s*\\}\", \"\", text)  # D\n",
    "    text = re.sub(r\"\\^\\s*\\\\circ\", \"\", text)  # E\n",
    "    text = text.replace(\"°\", \"\")  # E\n",
    "    match = re.match(r\"^\\\\text\\{(?P<x>.+?)\\}$\", text)  # F\n",
    "    if match:\n",
    "        text = match.group(\"x\")\n",
    "    text = re.sub(r\"\\\\\\(|\\\\\\)|\\\\\\[|\\\\\\]\", \"\", text)  # G\n",
    "    for pat, rep in LATEX_FIXES:  # H\n",
    "        text = re.sub(pat, rep, text)\n",
    "    \n",
    "    def convert_superscripts(s, base=None):\n",
    "        converted = \"\".join(\n",
    "            SUPERSCRIPT_MAP[ch] if ch in SUPERSCRIPT_MAP else ch\n",
    "            for ch in s\n",
    "        )\n",
    "        if base is None:\n",
    "            return converted\n",
    "        return f\"{base}**{converted}\"\n",
    "    \n",
    "    text = re.sub(\n",
    "        r\"([0-9A-Za-z\\)\\]\\}])([⁰¹²³⁴⁵⁶⁷⁸⁹⁺⁻]+)\",\n",
    "        lambda m: convert_superscripts(m.group(2), base=m.group(1)),\n",
    "        text,\n",
    "    )\n",
    "    text = convert_superscripts(text)\n",
    "    # I\n",
    "    text = text.replace(\"\\\\%\", \"%\").replace(\"$\", \"\").replace(\"%\", \"\")\n",
    "    text = re.sub(\n",
    "        r\"\\\\sqrt\\s*\\{([^}]*)\\}\",\n",
    "        lambda match: f\"sqrt({match.group(1)})\",\n",
    "        text,\n",
    "    )\n",
    "    text = re.sub(\n",
    "        r\"\\\\sqrt\\s+([^\\\\\\s{}]+)\",\n",
    "        lambda match: f\"sqrt({match.group(1)})\",\n",
    "        text,\n",
    "    )\n",
    "    # J\n",
    "    text = re.sub(\n",
    "        r\"\\\\frac\\s*\\{([^{}]+)\\}\\s*\\{([^{}]+)\\}\",\n",
    "        lambda match: f\"({match.group(1)})/({match.group(2)})\",\n",
    "        text,\n",
    "    )\n",
    "    text = re.sub(\n",
    "        r\"\\\\frac\\s+([^\\s{}]+)\\s+([^\\s{}]+)\",\n",
    "        lambda match: f\"({match.group(1)})/({match.group(2)})\",\n",
    "        text,\n",
    "    )\n",
    "    # K\n",
    "    text = text.replace(\"^\", \"**\")\n",
    "    text = re.sub(\n",
    "        r\"(?<=\\d)\\s+(\\d+/\\d+)\",\n",
    "        lambda match: \"+\" + match.group(1),\n",
    "        text,\n",
    "    )\n",
    "    # L\n",
    "    text = re.sub(\n",
    "        r\"(?<=\\d),(?=\\d\\d\\d(\\D|$))\",\n",
    "        \"\",\n",
    "        text,\n",
    "    )\n",
    "    return text.replace(\"{\", \"\").replace(\"}\", \"\").strip().lower()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3ac40f8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14)/(3)\n"
     ]
    }
   ],
   "source": [
    "print(normalize_text(extract_final_candidate(model_answer)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "79a8226b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14)/(3)\n"
     ]
    }
   ],
   "source": [
    "print(normalize_text(r\"\\text{\\[\\frac{14}{3}\\]}\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e47d28a",
   "metadata": {},
   "source": [
    "We can now verify the mathematical equivalence between the extracted answer from LLm and a GT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b9307ca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sympy.parsing import sympy_parser as spp\n",
    "from sympy.core.sympify import SympifyError\n",
    "from sympy.polys.polyerrors import PolynomialError\n",
    "from tokenize import TokenError\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "11c79e60",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sympy_parser(expr):\n",
    "    try:\n",
    "        return spp.parse_expr(expr, transformations=(\n",
    "\n",
    "            *spp.standard_transformations, #like handling parenthesis invariant symbols\n",
    "            spp.implicit_multiplication_application, #allow omitted mul symbols, like 2y == 2*y\n",
    "\n",
    "            ),\n",
    "        evaluate=True,\n",
    "        )\n",
    "    except (SympifyError, SyntaxError, TypeError, AttributeError,\n",
    "            IndexError, TokenError, ValueError, PolynomialError):\n",
    "        return None\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56c2540a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14/3\n"
     ]
    }
   ],
   "source": [
    "print(sympy_parser(\n",
    "    normalize_text(\n",
    "        extract_final_candidate(\n",
    "            model_answer))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e9ffc5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14/3\n"
     ]
    }
   ],
   "source": [
    "print(sympy_parser(\"28/6\")) #normalized by sympy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc57c495",
   "metadata": {},
   "source": [
    "We can noe build the equality function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "aa605c37",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sympy import simplify\n",
    "\n",
    "def equality_check(expr_gt, expr_pred):\n",
    "\n",
    "    if expr_gt == expr_pred:\n",
    "        return True\n",
    "    \n",
    "    gt, pred = sympy_parser(expr_gt), sympy_parser(expr_pred)\n",
    "\n",
    "    if gt is not None and pred is not None:\n",
    "\n",
    "        try:\n",
    "            return simplify(gt - pred) == 0 #for example, 14/3 and 28/6\n",
    "        \n",
    "        except(SympifyError, TypeError):\n",
    "            return False\n",
    "    \n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "00751261",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(equality_check(\n",
    "    normalize_text(\"13/4.\"),\n",
    "    normalize_text(r\"(13)/(4)\")\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "278d9685",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(equality_check(\n",
    "    normalize_text(\"0.5\"),\n",
    "    normalize_text(r\"(1)/(2)\")\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "459a20a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "print(equality_check(\n",
    "    normalize_text(\"14/3\"),\n",
    "    normalize_text(\"15/3\")\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c50b28eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "print(equality_check(\n",
    "    normalize_text(\"(14/3, 2/3)\"),\n",
    "    normalize_text(\"(14/3, 4/6)\")\n",
    "))\n",
    "#it cannot handle tuples as of now"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
