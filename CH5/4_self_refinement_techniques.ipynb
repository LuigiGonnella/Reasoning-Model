{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2b08d119",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from reasoning_from_scratch.ch02 import get_device\n",
    "from reasoning_from_scratch.ch03 import (\n",
    "load_model_and_tokenizer\n",
    ")\n",
    "from reasoning_from_scratch.ch03 import render_prompt\n",
    "from reasoning_from_scratch.ch04 import (\n",
    "generate_text_stream_concat_flex,\n",
    "generate_text_top_p_stream_cache\n",
    ")\n",
    "from reasoning_from_scratch.ch03 import extract_final_candidate\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "58ab2cc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ ..\\models\\qwen3_base\\qwen3-0.6B-base.pth already up-to-date\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\PersonalStudy\\OTHER_COURSES\\BOOKS\\General_Theory\\LLM\\LLM_second_course\\REASONING_MODELS\\Reasoning-Model\\venv\\Lib\\site-packages\\reasoning_from_scratch\\ch03.py:82: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(model_path))\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cpu\")\n",
    "\n",
    "model, tokenizer = load_model_and_tokenizer('base', device, use_compile=False, local_dir='../models/qwen3_base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "45f42917",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To solve the problem, we'll follow these steps:\n",
      "\n",
      "1. **Understand the problem**: We are given that half the value of \\( 3x - 9 \\) is equal to \\( x + 37 \\). We need to find the value of \\( x \\).\n",
      "\n",
      "2. **Translate the problem into an equation**:\n",
      "   - Half the value of \\( 3x - 9 \\) is \\( \\frac{1}{2}(3x - 9) \\).\n",
      "   - This is equal to \\( x + 37 \\).\n",
      "\n",
      "   So, the equation becomes:\n",
      "   \\[\n",
      "   \\frac{1}{2}(3x - 9) = x + 37\n",
      "   \\]\n",
      "\n",
      "3. **Simplify the equation**:\n",
      "   - Distribute the \\( \\frac{1}{2} \\) on the left side:\n",
      "     \\[\n",
      "     \\frac{1}{2} \\cdot 3x - \\frac{1}{2} \\cdot 9 = x + 37\n",
      "     \\]\n",
      "     \\[\n",
      "     \\frac{3x}{2} - \\frac{9}{2} = x + 37\n",
      "     \\]\n",
      "\n",
      "4. **Eliminate the fraction**:\n",
      "   - Multiply every term by 2 to eliminate the denominator:\n",
      "     \\[\n",
      "     2 \\cdot \\frac{3x}{2} - 2 \\cdot \\frac{9}{2} = 2 \\cdot x + 2 \\cdot 37\n",
      "     \\]\n",
      "     \\[\n",
      "     3x - 9 = 2x + 74\n",
      "     \\]\n",
      "\n",
      "5. **Isolate \\( x \\)**:\n",
      "   - Subtract \\( 2x \\) from both sides to get all \\( x \\)-terms on one side:\n",
      "     \\[\n",
      "     3x - 2x - 9 = 74\n",
      "     \\]\n",
      "     \\[\n",
      "     x - 9 = 74\n",
      "     \\]\n",
      "\n",
      "6. **Solve for \\( x \\)**:\n",
      "   - Add 9 to both sides to isolate \\( x \\):\n",
      "     \\[\n",
      "     x = 74 + 9\n",
      "     \\]\n",
      "     \\[\n",
      "     x = 83\n",
      "     \\]\n",
      "\n",
      "7. **Final Answer**:\n",
      "   \\[\n",
      "   \\boxed{83}\n",
      "   \\]"
     ]
    }
   ],
   "source": [
    "raw_prompt = (\n",
    "\"Half the value of $3x-9$ is $x+37$. \"\n",
    "\"What is the value of $x$?\"\n",
    ")\n",
    "\n",
    "prompt = render_prompt(raw_prompt)\n",
    "prompt_cot = prompt + '\\n\\nExplain step by step.\\n'\n",
    "\n",
    "torch.manual_seed(0)\n",
    "response_1 = generate_text_stream_concat_flex(\n",
    "    model, tokenizer, prompt_cot, device,\n",
    "    max_new_tokens=2048, verbose=True,\n",
    "    generate_func=generate_text_top_p_stream_cache,\n",
    "    temperature=0.9,\n",
    "    top_p=0.9\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "655ed842",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We are given the equation:\n",
      "\n",
      "\\[\n",
      "\\frac{1}{2}(3x - 9) = x + 37\n",
      "\\]\n",
      "\n",
      "**Step 1: Multiply both sides by 2 to eliminate the fraction.**\n",
      "\n",
      "\\[\n",
      "2 \\cdot \\frac{1}{2}(3x - 9) = 2(x + 37)\n",
      "\\]\n",
      "\n",
      "Simplifying:\n",
      "\n",
      "\\[\n",
      "3x - 9 = 2x + 74\n",
      "\\]\n",
      "\n",
      "**Step 2: Subtract \\(2x\\) from both sides to get all \\(x\\)-terms on one side.**\n",
      "\n",
      "\\[\n",
      "3x - 2x - 9 = 74\n",
      "\\]\n",
      "\n",
      "Simplifying:\n",
      "\n",
      "\\[\n",
      "x - 9 = 74\n",
      "\\]\n",
      "\n",
      "**Step 3: Add 9 to both sides to solve for \\(x\\).**\n",
      "\n",
      "\\[\n",
      "x = 74 + 9\n",
      "\\]\n",
      "\n",
      "Simplifying:\n",
      "\n",
      "\\[\n",
      "x = 83\n",
      "\\]\n",
      "\n",
      "**Final Answer:**\n",
      "\n",
      "\\[\n",
      "\\boxed{83}\n",
      "\\]"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(3)\n",
    "response_2 = generate_text_stream_concat_flex(\n",
    "    model, tokenizer, prompt_cot, device,\n",
    "    max_new_tokens=2048, verbose=True,\n",
    "    generate_func=generate_text_top_p_stream_cache,\n",
    "    temperature=0.9,\n",
    "    top_p=0.9\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ae2e7e67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response_1 characters --> 1204\n",
      "response_1 tokens --> 482\n",
      "\n",
      "response_2 characters --> 485\n",
      "response_2 tokens --> 220\n"
     ]
    }
   ],
   "source": [
    "#Different answers, same result.\n",
    "#We can demsntrate the second answer is the shortest\n",
    "\n",
    "print(f'response_1 characters --> {len(response_1)}')\n",
    "print(f'response_1 tokens --> {len(tokenizer.encode(response_1))}\\n')\n",
    "print(f'response_2 characters --> {len(response_2)}')\n",
    "print(f'response_2 tokens --> {len(tokenizer.encode(response_2))}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "540320a9",
   "metadata": {},
   "source": [
    "If two responses are comparable, then the one that is shorter is preferable, because wastes less tokens. But overall is not alqays the case that shorter answers are the best ones, it depends on human preference, and evaluating an answer is an open field of research (reward models not always pay off)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c96e94d8",
   "metadata": {},
   "source": [
    "# Rule-based score (heuristic)\n",
    "We give a score based on form and preference, not correctness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "22e0e282",
   "metadata": {},
   "outputs": [],
   "source": [
    "def heuristic_score(\n",
    "    answer,\n",
    "    prompt=None, #A\n",
    "    brevity_bonus=500.0,\n",
    "    boxed_bonus=2.0,\n",
    "    extract_bonus=1.0,\n",
    "    fulltext_bonus=0.0,):\n",
    "    score = 0.0\n",
    "\n",
    "    cand = extract_final_candidate(answer, fallback='none') #if the returned form is 'boxed' --> bonus score\n",
    "\n",
    "    if cand:\n",
    "        score += boxed_bonus\n",
    "    else:\n",
    "        cand = extract_final_candidate(answer, fallback='number_only') #if the returned form is just a number\n",
    "        if cand:\n",
    "            score += extract_bonus\n",
    "        else:\n",
    "            cand = extract_final_candidate(answer, fallback='number_then_full') #if it's not a number\n",
    "            if cand:\n",
    "                score += fulltext_bonus\n",
    "    \n",
    "    score += 1.5 * math.exp(-len(answer) / brevity_bonus) #we give a reward for brevity\n",
    "    return score\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "270d1a65",
   "metadata": {},
   "source": [
    "We can see the berivity_bonus effect with the following graphic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "029e7520",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "def plot_brevity_curve(brevity_bonus, max_len=2048):\n",
    "    lengths = torch.arange(1, max_len)\n",
    "    scores = 1.5 * torch.exp(-lengths / brevity_bonus)\n",
    "    plt.figure(figsize=(4, 3))\n",
    "    plt.plot(lengths, scores)\n",
    "    plt.xlabel(\"Text length (number of characters)\")\n",
    "    plt.ylabel(\"Score contribution\")\n",
    "    plt.tight_layout()\n",
    "    #plt.savefig(\"brevity_curve.pdf\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60e39915",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 400x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_brevity_curve(500) #we can see that 500 is not that high, with 500 it will increase by nearly 0.4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fb809c0",
   "metadata": {},
   "source": [
    "Let's evaluate our 2 responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b627a2c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.135\n"
     ]
    }
   ],
   "source": [
    "print(round(heuristic_score(response_1), 3)) #n_digits = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9bf25fe3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.569\n"
     ]
    }
   ],
   "source": [
    "print(round(heuristic_score(response_2), 3)) #n_digits = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77e970ed",
   "metadata": {},
   "source": [
    "We can use this function as a tie-breaker in the self-consistency loop to pick the best answer (preferable answer) among the one that are tied, or even as the only metric to pick the best answer instead of the majority voting --> in this case instead of Self Consistency we would call the method Best-of-N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "af494a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.inference_mode()\n",
    "def calc_next_token_probas(model, tokenizer, prompt, device):\n",
    "    model.eval()\n",
    "\n",
    "    token_ids = torch.tensor(tokenizer.encode(prompt), device=device).unsqueeze(0)\n",
    "    logits = model(token_ids).squeeze(0)\n",
    "    all_probas = torch.softmax(logits, dim=-1)\n",
    "\n",
    "    t_idxs = torch.arange(0, token_ids.shape[1] - 1, device=device)\n",
    "    next_idxs = token_ids.squeeze(0)[1:]\n",
    "\n",
    "    next_token_probas = all_probas[t_idxs, next_idxs]\n",
    "\n",
    "    print(\n",
    "        \"Next-token probabilities:\",\n",
    "        [p.item() for p in next_token_probas]\n",
    "        )\n",
    "    print(\n",
    "        \"Joint probability:\",\n",
    "        torch.prod(next_token_probas) \n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e5ea214d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Next-token probabilities: [5.745887756347656e-05, 0.453125, 0.016357421875, 0.7421875, 0.162109375]\n",
      "Joint probability: tensor(5.0990e-08, dtype=torch.bfloat16)\n"
     ]
    }
   ],
   "source": [
    "calc_next_token_probas(\n",
    "    model, tokenizer, device=device,\n",
    "    prompt=\"The capital of Germany is Berlin\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5f72b01",
   "metadata": {},
   "source": [
    "Let's try another text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8ffbb606",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Next-token probabilities: [5.745887756347656e-05, 0.453125, 0.016357421875, 0.7421875, 3.03611159324646e-07]\n",
      "Joint probability: tensor(9.5479e-14, dtype=torch.bfloat16)\n"
     ]
    }
   ],
   "source": [
    "calc_next_token_probas(\n",
    "    model, tokenizer, device=device,\n",
    "    prompt=\"The capital of Germany is Bridge\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db0d0d09",
   "metadata": {},
   "source": [
    "To handle this short numbers we can introduce log-probabilities --> more numerically stable (sums instead of prods and larger numbers, negatives, up to 0 for the highiest probability, which is 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ec82c669",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.inference_mode()\n",
    "def calc_next_token_logprobas(model, tokenizer, prompt, device):\n",
    "    model.eval()\n",
    "\n",
    "    token_ids = torch.tensor(tokenizer.encode(prompt), device=device).unsqueeze(0)\n",
    "    logits = model(token_ids).squeeze(0)\n",
    "    all_logprobas = torch.log_softmax(logits, dim=-1)\n",
    "\n",
    "    t_idxs = torch.arange(0, token_ids.shape[1] - 1, device=device)\n",
    "    next_idxs = token_ids.squeeze(0)[1:]\n",
    "\n",
    "    next_token_logprobas = all_logprobas[t_idxs, next_idxs]\n",
    "\n",
    "    print(\n",
    "        \"Next-token log-probabilities:\",\n",
    "        [p.item() for p in next_token_logprobas]\n",
    "        )\n",
    "    print(\n",
    "        \"Joint probability:\",\n",
    "        torch.sum(next_token_logprobas) \n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "545d1826",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Next-token log-probabilities: [-9.75, -0.796875, -4.125, -0.294921875, -1.8203125]\n",
      "Joint probability: tensor(-16.7500, dtype=torch.bfloat16)\n"
     ]
    }
   ],
   "source": [
    "calc_next_token_logprobas(\n",
    "    model, tokenizer, device=device,\n",
    "    prompt=\"The capital of Germany is Berlin\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b66f53cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Next-token log-probabilities: [-9.75, -0.796875, -4.125, -0.294921875, -15.0]\n",
      "Joint probability: tensor(-30., dtype=torch.bfloat16)\n"
     ]
    }
   ],
   "source": [
    "calc_next_token_logprobas(\n",
    "    model, tokenizer, device=device,\n",
    "    prompt=\"The capital of Germany is Bridge\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "870a42f0",
   "metadata": {},
   "source": [
    "We can use the average of log-probabilities (among the only answer, not the prompt) as a ranking value to evaluate the LLM confidence of a response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "61915bcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.inference_mode()\n",
    "def avg_logprob_answer(model, tokenizer, prompt, answer, device='cpu'):\n",
    "    model.eval()\n",
    "\n",
    "    prompt_ids = tokenizer.encode(prompt) \n",
    "    answer_ids = tokenizer.encode(answer) \n",
    "\n",
    "    token_ids = torch.tensor(prompt_ids + answer_ids, device=device).unsqueeze(0)\n",
    "\n",
    "    start_answer = len(prompt_ids) - 1\n",
    "    end_answer = token_ids.shape[1] - 1\n",
    "\n",
    "    logits = model(token_ids).squeeze(0)\n",
    "    all_logprobas = torch.log_softmax(logits, dim=-1)\n",
    "\n",
    "    t_idxs = torch.arange(start_answer, end_answer, device=device)\n",
    "    next_idxs = token_ids.squeeze(0)[start_answer + 1: end_answer + 1]\n",
    "\n",
    "    next_token_logprobas = all_logprobas[t_idxs, next_idxs]\n",
    "\n",
    "    return torch.mean(next_token_logprobas).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e3c3757f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.205078125\n"
     ]
    }
   ],
   "source": [
    "score_1 = avg_logprob_answer(\n",
    "    model, tokenizer,\n",
    "    prompt=\"What is the capital of Germany?\",\n",
    "    answer=\" The capital of Germany is Berlin.\",\n",
    "    device=device\n",
    ")\n",
    "print(score_1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "46b47cfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-3.890625\n"
     ]
    }
   ],
   "source": [
    "score_1 = avg_logprob_answer(\n",
    "    model, tokenizer,\n",
    "    prompt=\"What is the capital of Germany?\",\n",
    "    answer=\" The capital of Germany is Bridge.\",\n",
    "    device=device\n",
    ")\n",
    "print(score_1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b80f2e0d",
   "metadata": {},
   "source": [
    "# Self-Refinement Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e7bd01a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \\boxed{12}"
     ]
    }
   ],
   "source": [
    "raw_prompt = (\n",
    "\"Half the value of $3x-9$ is $x+37$. \"\n",
    "\"What is the value of $x$?\"\n",
    ")\n",
    "prompt = render_prompt(raw_prompt)\n",
    "\n",
    "torch.manual_seed(123)\n",
    "initial_response = generate_text_stream_concat_flex(\n",
    "    model, tokenizer, prompt, device,\n",
    "    max_new_tokens=2048, verbose=True,\n",
    "    generate_func=generate_text_top_p_stream_cache,\n",
    "    temperature=0.7,\n",
    "    top_p=0.9,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b379dc61",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_critique_prompt(raw_prompt, initial_response):\n",
    "    return (\n",
    "        \"You are a meticulous reviewer. Identify logical errors, missing \"\n",
    "        \"steps, or arithmetic mistakes. If the answer seems correct, \"\n",
    "        \"say so briefly. Then propose a concise plan to fix issues.\\n\\n\"\n",
    "        f\"Question:\\n{raw_prompt}\\n\\n\"\n",
    "        f\"Draft answer:\\n{initial_response}\\n\\n\"\n",
    "        \"Write a short critique and bullet-point fix plan \"\n",
    "        \"(under ~120 words).\\n\"\n",
    "        \"Critique:\"\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "41e6eda3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The problem statement is incomplete. It is unclear what is being asked for and what the conditions are.\n",
      "\n",
      "Fix plan: \n",
      "1. Read the problem statement carefully and identify the key information and what is being asked for.\n",
      "2. Identify the conditions given in the problem statement.\n",
      "3. Use the conditions to solve for the unknown variable $x$.\n",
      "4. Check your answer to ensure it satisfies the conditions given in the problem statement.\n",
      "5. Provide the final answer.\n",
      "\n",
      "Solution:\n",
      "Let's solve the problem step by step:\n",
      "\n",
      "1. The problem states that half the value of $3x-9$ is $x+37$.\n",
      "2. We can write this as: $\\frac{1}{2}(3x-9) = x+37$.\n",
      "3. To solve for $x$, we can multiply both sides of the equation by 2 to eliminate the fraction: $3x-9 = 2(x+37)$.\n",
      "4. Simplify the equation: $3x-9 = 2x+74$.\n",
      "5. Subtract $2x$ from both sides: $x-9 = 74$.\n",
      "6. Add 9 to both sides: $x = 83$.\n",
      "\n",
      "The value of $x$ is 83."
     ]
    }
   ],
   "source": [
    "critique_prompt = make_critique_prompt(raw_prompt, initial_response)\n",
    "torch.manual_seed(123)\n",
    "critique = generate_text_stream_concat_flex(\n",
    "    model, tokenizer, critique_prompt, device,\n",
    "    max_new_tokens=2048, verbose=True,\n",
    "    generate_func=generate_text_top_p_stream_cache,\n",
    "    temperature=0.7,\n",
    "    top_p=0.9,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "15edf5c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_refinement_prompt(raw_prompt, initial_response, critique):\n",
    "    return (\n",
    "        \"Revise the answer using the critique. Keep it concise and \"\n",
    "        \"end with a final boxed result: \\\\boxed{ANSWER}\\n\\n\"\n",
    "        f\"Question:\\n{raw_prompt}\\n\\n\"\n",
    "        f\"Previous answer:\\n{initial_response}\\n\\n\"\n",
    "        f\"Critique:\\n{critique}\\n\\n\"\n",
    "        \"Revised answer:\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a5f8a6db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \\boxed{83}"
     ]
    }
   ],
   "source": [
    "refinement_prompt = make_refinement_prompt(raw_prompt, initial_response, critique)\n",
    "\n",
    "torch.manual_seed(123)\n",
    "final_response = generate_text_stream_concat_flex(\n",
    "    model, tokenizer, refinement_prompt, device,\n",
    "    max_new_tokens=2048, verbose=True,\n",
    "    generate_func=generate_text_top_p_stream_cache,\n",
    "    temperature=0.7,\n",
    "    top_p=0.9,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4c19654",
   "metadata": {},
   "source": [
    "We can feed this process in the self-refinement function to automatize it. We can also make possible to reiterate the loop on the revised answer for a fixed number of iterations or, better, until the final response reach a target score (e.g. calculated with avergae_log_probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "cb313d38",
   "metadata": {},
   "outputs": [],
   "source": [
    "def self_refinement_loop(\n",
    "        model,\n",
    "        tokenizer,\n",
    "        raw_prompt,\n",
    "        device,\n",
    "        iterations=2,\n",
    "        max_response_tokens=2048,\n",
    "        max_critique_tokens=256,\n",
    "        score_fn=None,\n",
    "        prompt_renderer=render_prompt,\n",
    "        prompt_suffix=\"\",\n",
    "        verbose=False,\n",
    "        temperature=0.7,\n",
    "        top_p=0.9,\n",
    "    ):\n",
    "\n",
    "    steps = []\n",
    "    prompt = prompt_renderer(raw_prompt) + prompt_suffix\n",
    "    current_full = generate_text_stream_concat_flex(\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        prompt=prompt,\n",
    "        device=device,\n",
    "        max_new_tokens=max_response_tokens,\n",
    "        verbose=False,\n",
    "        generate_func=generate_text_top_p_stream_cache,\n",
    "        temperature=temperature,\n",
    "        top_p=top_p,\n",
    "    )\n",
    "\n",
    "    current_extracted = extract_final_candidate(\n",
    "        current_full, fallback=\"number_then_full\"\n",
    "    )\n",
    "\n",
    "    if score_fn:\n",
    "        current_score = score_fn(answer=current_full, prompt=prompt)\n",
    "    else:\n",
    "        current_score = 0.0\n",
    "    \n",
    "    for iter in range(iterations):\n",
    "\n",
    "        draft_before_full = current_full\n",
    "        draft_before_extracted = current_extracted\n",
    "        score_before = current_score\n",
    "\n",
    "        critique_prompt = make_critique_prompt(raw_prompt, draft_before_full)\n",
    "        critique_full = generate_text_stream_concat_flex(\n",
    "            model=model,\n",
    "            tokenizer=tokenizer,\n",
    "            prompt=critique_prompt,\n",
    "            device=device,\n",
    "            max_new_tokens=max_critique_tokens,\n",
    "            verbose=False,\n",
    "            generate_func=generate_text_top_p_stream_cache,\n",
    "            temperature=temperature,\n",
    "            top_p=top_p,\n",
    "        )\n",
    "\n",
    "        refinement_prompt = make_refinement_prompt(raw_prompt, draft_before_full, critique_full)\n",
    "        refinement_full = generate_text_stream_concat_flex(\n",
    "            model=model,\n",
    "            tokenizer=tokenizer,\n",
    "            prompt=refinement_prompt,\n",
    "            device=device,\n",
    "            max_new_tokens=max_response_tokens,\n",
    "            verbose=False,\n",
    "            generate_func=generate_text_top_p_stream_cache,\n",
    "            temperature=temperature,\n",
    "            top_p=top_p,\n",
    "        )\n",
    "        refinement_extracted = extract_final_candidate(\n",
    "            refinement_full, fallback=\"number_then_full\"\n",
    "        )\n",
    "\n",
    "        if score_fn:\n",
    "            refinement_score = score_fn(answer=refinement_full, prompt=refinement_prompt)\n",
    "        else:\n",
    "            refinement_score = 0.0\n",
    "\n",
    "        step = {\n",
    "            \"iteration\": iter + 1,\n",
    "            \"draft_full\": draft_before_full,\n",
    "            \"draft_extracted\": draft_before_extracted,\n",
    "            \"critique\": critique_full,\n",
    "            \"revised_full\": refinement_full,\n",
    "            \"revised_extracted\": refinement_extracted,\n",
    "            \"score_before\": score_before,\n",
    "            \"score_after\": refinement_score,\n",
    "            }\n",
    "        steps.append(step)\n",
    "\n",
    "        if verbose:\n",
    "            print(\n",
    "                f\"[Refinement {iter+1}/{iterations}]\"\n",
    "                f\"\\nCurrent: {draft_before_extracted}\"\n",
    "                f\"\\nRevised: {refinement_extracted}\"\n",
    "                f\"\\nScore before: {score_before:.3f}\"\n",
    "                f\"\\nScore after: {refinement_score:.3f}\"\n",
    "                f\"\\n{'=' * 25}\\n\"\n",
    "            )\n",
    "\n",
    "\n",
    "\n",
    "        if refinement_score > score_before:\n",
    "            current_score = refinement_score\n",
    "            current_full = refinement_full\n",
    "            current_extracted = refinement_extracted\n",
    "\n",
    "    return {\n",
    "        \"final_full\": current_full,\n",
    "        \"final_extracted\": current_extracted,\n",
    "        \"steps\": steps,\n",
    "    }\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "35fb54a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "avg_logprob_score = partial(\n",
    "        avg_logprob_answer,\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        device=device\n",
    "    ) #specified fields remain fixed so that we can pass to the function only 2 parameters (prompt and answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "ea5e1750",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Refinement 1/2]\n",
      "Current: 38\n",
      "Revised: 83\n",
      "Score before: -0.941\n",
      "Score after: -0.199\n",
      "=========================\n",
      "\n",
      "[Refinement 2/2]\n",
      "Current: 83\n",
      "Revised: 83\n",
      "Score before: -0.199\n",
      "Score after: -0.170\n",
      "=========================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "results_logprob = self_refinement_loop(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    raw_prompt=raw_prompt,\n",
    "    device=device,\n",
    "    iterations=2,\n",
    "    max_response_tokens=2048,\n",
    "    max_critique_tokens=256,\n",
    "    score_fn=avg_logprob_score,\n",
    "    verbose=True,\n",
    "    temperature=0.7,\n",
    "    top_p=0.9,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "e39d4763",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' \\\\boxed{83}'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_logprob['final_full']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0a4bf9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' \\\\boxed{83}\\n\\nCorrected critique:\\nThe solution is now correct. The equation $3x-9 = 2(x+37)$ simplifies to $3x-9 = 2x+74$, which further simplifies to $x = 83$. However, the original equation should have been $3x-9 = x+37$, leading to the correct solution $x = 38$.'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_logprob['steps'][0]['revised_full'] #longer, and bigger avg_log_probas "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
