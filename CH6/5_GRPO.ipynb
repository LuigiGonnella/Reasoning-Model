{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d7cabf01",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from reasoning_from_scratch.ch02 import get_device\n",
    "from reasoning_from_scratch.ch03 import load_model_and_tokenizer\n",
    "from reasoning_from_scratch.ch03 import render_prompt\n",
    "from reasoning_from_scratch.ch04 import (\n",
    "generate_text_stream_concat_flex,\n",
    "generate_text_top_p_stream_cache\n",
    ")\n",
    "import json\n",
    "from pathlib import Path\n",
    "import requests\n",
    "from pprint import pprint\n",
    "from reasoning_from_scratch.qwen3 import KVCache\n",
    "from reasoning_from_scratch.ch04 import top_p_filter\n",
    "from reasoning_from_scratch.ch03 import (\n",
    "extract_final_candidate, grade_answer\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d0216079",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using NVIDIA CUDA GPU\n",
      "âœ“ ..\\models\\qwen3_base\\qwen3-0.6B-base.pth already up-to-date\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\PersonalStudy\\OTHER_COURSES\\BOOKS\\General_Theory\\LLM\\LLM_second_course\\REASONING_MODELS\\Reasoning-Model\\venv\\Lib\\site-packages\\reasoning_from_scratch\\ch03.py:82: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(model_path))\n"
     ]
    }
   ],
   "source": [
    "device = get_device()\n",
    "model, tokenizer = load_model_and_tokenizer('base', device, local_dir='../models/qwen3_base', use_compile=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eca80f9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \\boxed{38}"
     ]
    }
   ],
   "source": [
    "raw_prompt =  (\n",
    "\"Half the value of $3x-9$ is $x+37$. \"\n",
    "\"What is the value of $x$?\"\n",
    ")\n",
    "\n",
    "prompt = render_prompt(raw_prompt)\n",
    "\n",
    "torch.manual_seed(0)\n",
    "response = generate_text_stream_concat_flex(model,\n",
    "                                            tokenizer, prompt, device, max_new_tokens=2048, verbose=True, \n",
    "                                            generate_func=generate_text_top_p_stream_cache,\n",
    "                                            temperature = 0.9, top_p = 0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fbcca309",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_math_train(local_path = 'math_train.json', save_copy=True):\n",
    "    local_path = Path(local_path)\n",
    "    url = (\n",
    "        \"https://raw.githubusercontent.com/rasbt/\"\n",
    "        \"math_full_minus_math500/refs/heads/main/\"\n",
    "        \"math_full_minus_math500.json\"\n",
    "    )\n",
    "\n",
    "    if local_path.exists():\n",
    "        with local_path.open('r', encoding='utf-8') as f:\n",
    "            data = json.load(f)\n",
    "    else:\n",
    "        r = requests.get(url, timeout=30)\n",
    "        r.raise_for_status()\n",
    "        data = r.json()\n",
    "\n",
    "        if save_copy:\n",
    "            with local_path.open('w', encoding='utf-8') as f:\n",
    "                json.dump(data, f, indent=2)\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "40ef7dba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: 12000\n"
     ]
    }
   ],
   "source": [
    "math_train = load_math_train(save_copy=True)\n",
    "print(f'Dataset size: {len(math_train)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "91f2abdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'answer': '6',\n",
      " 'level': 'Level 3',\n",
      " 'problem': 'Sam is hired for a 20-day period. On days that he works, he earns '\n",
      "            '$\\\\$$60. For each day that he does not work, $\\\\$$30 is '\n",
      "            'subtracted from his earnings. At the end of the 20-day period, he '\n",
      "            'received $\\\\$$660. How many days did he not work?',\n",
      " 'solution': 'Call $x$ the number of days Sam works and $y$ the number of days '\n",
      "             'he does not. We can set up the following system of equations to '\n",
      "             'represent the given information: \\\\begin{align*}\\n'\n",
      "             'x+y &= 20 \\\\\\\\\\n'\n",
      "             '60x - 30y &= 660 \\\\\\\\\\n'\n",
      "             '\\\\end{align*} The first equation represents the total number of '\n",
      "             'days Sam works, and the second equation represents his total '\n",
      "             'profit. Solving for $x$ in the first equation yields $x = 20 - '\n",
      "             'y$. Substituting into the second equation gives $60(20-y) - 30y '\n",
      "             '= 660$. Canceling a factor of $10$ and multiplying out gives '\n",
      "             '$120 - 6y - 3y = 66$. This simplifies to $-9y = -54$, or $y = '\n",
      "             '6$. Thus, Sam did not work for $\\\\boxed{6}$ days.',\n",
      " 'type': 'Algebra',\n",
      " 'unique_id': 4}\n"
     ]
    }
   ],
   "source": [
    "pprint(math_train[4])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7acef998",
   "metadata": {},
   "source": [
    "We could use 'solution' field to validate step-by-step reasoning of the model but this would lead to overfitting; instead, wewant the model to generalize its reasoning ability without memorizing this specific approach."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d49f07ee",
   "metadata": {},
   "source": [
    "# Implementing GRPO PHASES"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf7f52c3",
   "metadata": {},
   "source": [
    "**PHASE 1** \n",
    "_GENEARTING & SAMPLING ROLLOUTS_\n",
    "\n",
    "We could use the **generate_text_stream_concat_flex** to generate responses, but this was decorated with @inference_only(), making it useless for training purposes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6129d767",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def sample_response(\n",
    "    model, \n",
    "    tokenizer, \n",
    "    prompt, \n",
    "    device,\n",
    "    max_new_tokens=512, \n",
    "    temperature = 0.8,\n",
    "    top_p = 0.9\n",
    "):\n",
    "    input_ids = torch.tensor(tokenizer.encode(prompt), device=device)\n",
    "    cache = KVCache(n_layers=model.cfg['n_layers'])\n",
    "    model.reset_kv_cache()\n",
    "    next_token_logits = model(input_ids.unsqueeze(0), cache=cache)[:, -1]\n",
    "\n",
    "    generated = []\n",
    "    for _ in range(max_new_tokens):\n",
    "        if temperature and temperature != 1.0: \n",
    "            next_token_logits /= temperature\n",
    "\n",
    "        next_token_probas = torch.softmax(next_token_logits, dim=-1)\n",
    "        next_token_probas = top_p_filter(next_token_probas, top_p)\n",
    "        next_token_id = torch.multinomial(\n",
    "            next_token_probas.cpu(), num_samples=1\n",
    "        ).to(device)\n",
    "\n",
    "        if (tokenizer.eos_token_id is not None and next_token_id.item() == tokenizer.eos_token_id):\n",
    "            break\n",
    "\n",
    "        generated.append(next_token_id.item())\n",
    "        next_token_logits = model(next_token_id, cache=cache)[:, -1]\n",
    "    \n",
    "    full_token_ids = torch.cat([input_ids, torch.tensor(generated, device=device, dtype=input_ids.dtype)])\n",
    "\n",
    "    return full_token_ids, input_ids.numel(), tokenizer.decode(generated) #return full answer + prompt, number of token of prompt and generated text\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5d505371",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \\boxed{38}\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "raw_prompt = (\n",
    "\"Half the value of $3x-9$ is $x+37$. \"\n",
    "\"What is the value of $x$?\"\n",
    ")\n",
    "prompt = render_prompt(raw_prompt)\n",
    "token_ids, prompt_len, answer_text = sample_response(\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        prompt=prompt,\n",
    "        device=device,\n",
    "        max_new_tokens=512,\n",
    "        temperature=0.9,\n",
    "        top_p=0.9,\n",
    "    )\n",
    "print(answer_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d705fd46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' \\\\boxed{42}', ' To solve the equation, we start by simplifying the given equation.\\n\\n\\\\[\\n\\\\frac{1}{2}(3x - 9) = x + 37\\n\\\\]\\n\\nFirst, distribute the \\\\(\\\\frac{1}{2}\\\\) on the left side:\\n\\n\\\\[\\n\\\\frac{3x}{2} - \\\\frac{9}{2} = x + 37\\n\\\\]\\n\\nNext, eliminate the fraction by multiplying every term by 2:\\n\\n\\\\[\\n3x - 9 = 2x + 74\\n\\\\]\\n\\nNow, isolate \\\\(x\\\\) by subtracting \\\\(2x\\\\) from both sides:\\n\\n\\\\[\\n3x - 2x - 9 = 74\\n\\\\]\\n\\nSimplify:\\n\\n\\\\[\\nx - 9 = 74\\n\\\\]\\n\\nAdd 9 to both sides to solve for \\\\(x\\\\):\\n\\n\\\\[\\nx = 74 + 9\\n\\\\]\\n\\n\\\\[\\nx = 83\\n\\\\]\\n\\nThus, the value of \\\\(x\\\\) is \\\\(\\\\boxed{83}\\\\).', ' \\\\boxed{32}', ' To solve the equation \"Half the value of \\\\(3x - 9\\\\) is \\\\(x + 37\\\\)\", follow these steps:\\n\\n1. **Translate the problem into an equation:**\\n   \\n   \\\\[\\n   \\\\frac{1}{2}(3x - 9) = x + 37\\n   \\\\]\\n\\n2. **Eliminate the fraction by multiplying both sides by 2:**\\n   \\n   \\\\[\\n   2 \\\\times \\\\frac{1}{2}(3x - 9) = 2 \\\\times (x + 37)\\n   \\\\]\\n   \\n   \\\\[\\n   3x - 9 = 2x + 74\\n   \\\\]\\n\\n3. **Isolate the variable \\\\(x\\\\) by subtracting \\\\(2x\\\\) from both sides:**\\n   \\n   \\\\[\\n   3x - 2x - 9 = 74\\n   \\\\]\\n   \\n   \\\\[\\n   x - 9 = 74\\n   \\\\]\\n\\n4. **Solve for \\\\(x\\\\) by adding 9 to both sides:**\\n   \\n   \\\\[\\n   x = 74 + 9\\n   \\\\]\\n   \\n   \\\\[\\n   x = 83\\n   \\\\]\\n\\n5. **Final Answer:**\\n   \\n   \\\\[\\n   \\\\boxed{83}\\n   \\\\]']\n"
     ]
    }
   ],
   "source": [
    "rollouts = []\n",
    "\n",
    "for _ in range(4):\n",
    "    rollouts.append(sample_response(\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        prompt=prompt,\n",
    "        device=device,\n",
    "        max_new_tokens=512,\n",
    "        temperature=0.9,\n",
    "        top_p=0.9,\n",
    "    )[2])\n",
    "\n",
    "print(rollouts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53da2627",
   "metadata": {},
   "source": [
    "**PHASE 2** \n",
    "_CALCULATING REWARDS_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f10f860e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reward_rlvr(answer_text, gt_text):\n",
    "    extracted = extract_final_candidate(answer_text, fallback=None) #we require /boxed format\n",
    "    if not extracted: #not boxed, we can still assign 0.5 if the number was correct but not boxed\n",
    "        extracted_not_boxed = extract_final_candidate(answer_text, fallback='number_only')\n",
    "\n",
    "        return float(grade_answer(extracted_not_boxed, gt_text)) / 2\n",
    "            \n",
    "\n",
    "    correct = grade_answer(extracted, gt_text) #True or False\n",
    "    return float(correct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "82a24eab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: ' \\\\boxed{42}'\n",
      "Reward: 0.0\n",
      "\n",
      "Answer: ' To solve the equation, we start by simplifying the given equation.\\n\\n\\\\[\\n\\\\frac{1}{2}(3x - 9) = x + 37\\n\\\\]\\n\\nFirst, distribute the \\\\(\\\\frac{1}{2}\\\\) on the left side:\\n\\n\\\\[\\n\\\\frac{3x}{2} - \\\\frac{9}{2} = x + 37\\n\\\\]\\n\\nNext, eliminate the fraction by multiplying every term by 2:\\n\\n\\\\[\\n3x - 9 = 2x + 74\\n\\\\]\\n\\nNow, isolate \\\\(x\\\\) by subtracting \\\\(2x\\\\) from both sides:\\n\\n\\\\[\\n3x - 2x - 9 = 74\\n\\\\]\\n\\nSimplify:\\n\\n\\\\[\\nx - 9 = 74\\n\\\\]\\n\\nAdd 9 to both sides to solve for \\\\(x\\\\):\\n\\n\\\\[\\nx = 74 + 9\\n\\\\]\\n\\n\\\\[\\nx = 83\\n\\\\]\\n\\nThus, the value of \\\\(x\\\\) is \\\\(\\\\boxed{83}\\\\).'\n",
      "Reward: 1.0\n",
      "\n",
      "Answer: ' \\\\boxed{32}'\n",
      "Reward: 0.0\n",
      "\n",
      "Answer: ' To solve the equation \"Half the value of \\\\(3x - 9\\\\) is \\\\(x + 37\\\\)\", follow these steps:\\n\\n1. **Translate the problem into an equation:**\\n   \\n   \\\\[\\n   \\\\frac{1}{2}(3x - 9) = x + 37\\n   \\\\]\\n\\n2. **Eliminate the fraction by multiplying both sides by 2:**\\n   \\n   \\\\[\\n   2 \\\\times \\\\frac{1}{2}(3x - 9) = 2 \\\\times (x + 37)\\n   \\\\]\\n   \\n   \\\\[\\n   3x - 9 = 2x + 74\\n   \\\\]\\n\\n3. **Isolate the variable \\\\(x\\\\) by subtracting \\\\(2x\\\\) from both sides:**\\n   \\n   \\\\[\\n   3x - 2x - 9 = 74\\n   \\\\]\\n   \\n   \\\\[\\n   x - 9 = 74\\n   \\\\]\\n\\n4. **Solve for \\\\(x\\\\) by adding 9 to both sides:**\\n   \\n   \\\\[\\n   x = 74 + 9\\n   \\\\]\\n   \\n   \\\\[\\n   x = 83\\n   \\\\]\\n\\n5. **Final Answer:**\\n   \\n   \\\\[\\n   \\\\boxed{83}\\n   \\\\]'\n",
      "Reward: 1.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rollouts_rewards = []\n",
    "\n",
    "for rollout in rollouts:\n",
    "    reward = reward_rlvr(rollout, '83')\n",
    "    print(f\"Answer: {rollout!r}\")\n",
    "    print(f\"Reward: {reward}\\n\")\n",
    "\n",
    "    rollouts_rewards.append(reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78dff7b5",
   "metadata": {},
   "source": [
    "We could decide to train the model with an additional intermediate reward during the generation process (like PPO), but these experiments were unsuccessful"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e790b6e1",
   "metadata": {},
   "source": [
    "**PHASE 3** \n",
    "_CALCULATING ADVANTAGES_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "22e69d95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0., 1., 0., 1.], device='cuda:0')\n",
      "tensor([-0.8659,  0.8659, -0.8659,  0.8659], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "rewards = torch.tensor(rollouts_rewards, device=device)\n",
    "advantages = (rewards - torch.mean(rewards)) / (torch.std(rewards, dim=-1) + 1e-4)\n",
    "\n",
    "print(rewards)\n",
    "print(advantages)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc483dc9",
   "metadata": {},
   "source": [
    "**PHASE 4** \n",
    "_CALCULATING LOGPROBS_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e6d3adbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sequence_logprob_answer(model, tokenizer, prompt, answer, device='cpu'):\n",
    "\n",
    "    prompt_ids = tokenizer.encode(prompt)\n",
    "    answer_ids = tokenizer.encode(answer)\n",
    "    full_ids = torch.tensor(prompt_ids + answer_ids, device=device)\n",
    "\n",
    "    logits = model(full_ids.unsqueeze(0)).squeeze(0)\n",
    "    logprobs = torch.log_softmax(logits, dim=-1)\n",
    "\n",
    "    start = len(prompt_ids) - 1\n",
    "    end = full_ids.shape[0] - 1\n",
    "\n",
    "    t_idx = torch.arange(start, end, device=device)\n",
    "    next_tokens = full_ids[start + 1:end + 1]\n",
    "    next_token_logprobs = logprobs[t_idx, next_tokens]\n",
    "\n",
    "    return torch.sum(next_token_logprobs) #sum instead of mean, like it was for token-level logprob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "11a79cdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-5.7500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<SumBackward0>)\n"
     ]
    }
   ],
   "source": [
    "avg_logprob_val = sequence_logprob_answer(\n",
    "    model, tokenizer,\n",
    "    prompt=prompt,\n",
    "    answer=answer_text,\n",
    "    device=device)\n",
    "print(avg_logprob_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1f67097",
   "metadata": {},
   "source": [
    "We now build the same function, but already with token_ids and prompt_len as input fields, since they are geenratyed buy the **sample_response** function written before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0b195628",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sequence_logprob_draft(model, token_ids, prompt_len):\n",
    "\n",
    "    logits = model(token_ids.unsqueeze(0)).squeeze(0)\n",
    "    logprobs = torch.log_softmax(logits, dim=-1)\n",
    "\n",
    "    start = prompt_len - 1\n",
    "    end = token_ids.shape[0] - 1\n",
    "\n",
    "    t_idx = torch.arange(start, end, device=device)\n",
    "    next_tokens = token_ids[start + 1:end + 1]\n",
    "    next_token_logprobs = logprobs[t_idx, next_tokens]\n",
    "\n",
    "    return torch.sum(next_token_logprobs) #sum instead of mean, like it was for token-level logprob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "97c6876b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-5.7500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<SumBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(sequence_logprob_draft(model, token_ids, prompt_len))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc855c71",
   "metadata": {},
   "source": [
    "As we can see, the SumBackward0 entry is present, this shows that the summation of token level logprobs are part og the computational graph. We need exactly this when updating the model weights to minimize the policy gradient loss."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "488ccc4d",
   "metadata": {},
   "source": [
    "We can also implement a more efficient version of sequence_logprob, using **gather** function instead of manually indexig the input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "009b50e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sequence_logprob(model, token_ids, prompt_len):\n",
    "    logits = model(token_ids.unsqueeze(0)).squeeze(0).float()\n",
    "    logprobs = torch.log_softmax(logits, dim=-1)\n",
    "    \n",
    "    selected = logprobs[:-1].gather(\n",
    "        1, token_ids[1:].unsqueeze(-1)\n",
    "    ).squeeze(-1)\n",
    "\n",
    "    return torch.sum(selected[prompt_len - 1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d13c800a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-5.7268, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(sequence_logprob(model, token_ids, prompt_len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "3952eab6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer:  \\boxed{42}\n",
      "Logprob: -10.7715\n",
      "\n",
      "Answer:  To solve the equation, we start by simplifying the given equation.\n",
      "\n",
      "\\[\n",
      "\\frac{1}{2}(3x - 9) = x + 37\n",
      "\\]\n",
      "\n",
      "First, distribute the \\(\\frac{1}{2}\\) on the left side:\n",
      "\n",
      "\\[\n",
      "\\frac{3x}{2} - \\frac{9}{2} = x + 37\n",
      "\\]\n",
      "\n",
      "Next, eliminate the fraction by multiplying every term by 2:\n",
      "\n",
      "\\[\n",
      "3x - 9 = 2x + 74\n",
      "\\]\n",
      "\n",
      "Now, isolate \\(x\\) by subtracting \\(2x\\) from both sides:\n",
      "\n",
      "\\[\n",
      "3x - 2x - 9 = 74\n",
      "\\]\n",
      "\n",
      "Simplify:\n",
      "\n",
      "\\[\n",
      "x - 9 = 74\n",
      "\\]\n",
      "\n",
      "Add 9 to both sides to solve for \\(x\\):\n",
      "\n",
      "\\[\n",
      "x = 74 + 9\n",
      "\\]\n",
      "\n",
      "\\[\n",
      "x = 83\n",
      "\\]\n",
      "\n",
      "Thus, the value of \\(x\\) is \\(\\boxed{83}\\).\n",
      "Logprob: -36.6990\n",
      "\n",
      "Answer:  \\boxed{32}\n",
      "Logprob: -10.3265\n",
      "\n",
      "Answer:  To solve the equation \"Half the value of \\(3x - 9\\) is \\(x + 37\\)\", follow these steps:\n",
      "\n",
      "1. **Translate the problem into an equation:**\n",
      "   \n",
      "   \\[\n",
      "   \\frac{1}{2}(3x - 9) = x + 37\n",
      "   \\]\n",
      "\n",
      "2. **Eliminate the fraction by multiplying both sides by 2:**\n",
      "   \n",
      "   \\[\n",
      "   2 \\times \\frac{1}{2}(3x - 9) = 2 \\times (x + 37)\n",
      "   \\]\n",
      "   \n",
      "   \\[\n",
      "   3x - 9 = 2x + 74\n",
      "   \\]\n",
      "\n",
      "3. **Isolate the variable \\(x\\) by subtracting \\(2x\\) from both sides:**\n",
      "   \n",
      "   \\[\n",
      "   3x - 2x - 9 = 74\n",
      "   \\]\n",
      "   \n",
      "   \\[\n",
      "   x - 9 = 74\n",
      "   \\]\n",
      "\n",
      "4. **Solve for \\(x\\) by adding 9 to both sides:**\n",
      "   \n",
      "   \\[\n",
      "   x = 74 + 9\n",
      "   \\]\n",
      "   \n",
      "   \\[\n",
      "   x = 83\n",
      "   \\]\n",
      "\n",
      "5. **Final Answer:**\n",
      "   \n",
      "   \\[\n",
      "   \\boxed{83}\n",
      "   \\]\n",
      "Logprob: -26.8390\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rollout_logps = []\n",
    "\n",
    "for text in rollouts:\n",
    "    token_ids = tokenizer.encode(prompt + ' ' + text)\n",
    "    logprob = sequence_logprob(\n",
    "        model, token_ids=torch.tensor(token_ids, device=device),\n",
    "        prompt_len=prompt_len\n",
    "    )\n",
    "    print(f\"Answer: {text}\")\n",
    "    print(f\"Logprob: {logprob.item():.4f}\\n\")\n",
    "    rollout_logps.append(logprob)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f74c5817",
   "metadata": {},
   "source": [
    "**PHASE 5** \n",
    "_CALCULATING POLICY GRADIENTS LOSS_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "6d52b6af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-10.7715, -36.6990, -10.3265, -26.8390], device='cuda:0',\n",
      "       grad_fn=<StackBackward0>)\n",
      "tensor(9.1869, device='cuda:0', grad_fn=<NegBackward0>)\n"
     ]
    }
   ],
   "source": [
    "logps = torch.stack(rollout_logps) #transform to tensor\n",
    "pg_loss = - (advantages.detach() * logps).mean() #detach prevent the gradients from flowing back through the advanatage calculation, since it is a fixed parameter (not learnable)\n",
    "\n",
    "print(logps)\n",
    "print(pg_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef752d26",
   "metadata": {},
   "source": [
    "**PUTTING EVERYTHING TOGETHER** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0b7b9fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_grpo_loss(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    example,\n",
    "    device,\n",
    "    num_rollouts=2,\n",
    "    max_new_tokens=256,\n",
    "    temperature=0.8,\n",
    "    top_p=0.9,\n",
    "):\n",
    "    assert num_rollouts >= 2\n",
    "    roll_logps, roll_rewards, samples = [], [], []\n",
    "    prompt = render_prompt(example[\"problem\"]) #take the prompt\n",
    "    was_training = model.training\n",
    "    model.eval()\n",
    "    for _ in range(num_rollouts):\n",
    "\n",
    "        token_ids, prompt_len, text = sample_response(\n",
    "            model=model,\n",
    "            tokenizer=tokenizer,\n",
    "            prompt=prompt,\n",
    "            device=device,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            temperature=temperature,\n",
    "            top_p=top_p,\n",
    "        ) #sample a single response\n",
    "\n",
    "        reward = reward_rlvr(text, example[\"answer\"]) #calculate reward\n",
    "\n",
    "        logp = sequence_logprob(model, token_ids, prompt_len) #calculate sequence_logprobs\n",
    "        roll_logps.append(logp)\n",
    "        roll_rewards.append(reward)\n",
    "        samples.append(\n",
    "            {\n",
    "                \"text\": text,\n",
    "                \"reward\": reward,\n",
    "                \"gen_len\": token_ids.numel() - prompt_len,\n",
    "            }\n",
    "        )\n",
    "    if was_training:\n",
    "        model.train()\n",
    "\n",
    "    rewards = torch.tensor(roll_rewards, device=device)\n",
    "\n",
    "    advantages = (rewards - rewards.mean()) / (rewards.std() + 1e-4) #calculate advantages\n",
    "\n",
    "    logps = torch.stack(roll_logps)\n",
    "    \n",
    "    pg_loss = -(advantages.detach() * logps).mean() #calculate policy gradient loss\n",
    "    loss = pg_loss  # after we'll add a KL term here\n",
    "    return {\n",
    "        \"loss\": loss.item(),\n",
    "        \"pg_loss\": pg_loss.item(),\n",
    "        \"rewards\": roll_rewards,\n",
    "        \"advantages\": advantages.detach().cpu().tolist(),\n",
    "        \"samples\": samples,\n",
    "        \"loss_tensor\": loss,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "330e60f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'advantages': [0.0, 0.0],\n",
      " 'loss': -0.0,\n",
      " 'loss_tensor': tensor(-0., device='cuda:0', grad_fn=<NegBackward0>),\n",
      " 'pg_loss': -0.0,\n",
      " 'rewards': [0.0, 0.0],\n",
      " 'samples': [{'gen_len': 4, 'reward': 0.0, 'text': ' 10 days'},\n",
      "             {'gen_len': 256,\n",
      "              'reward': 0.0,\n",
      "              'text': ' \\n'\n",
      "                      'Sam earned a total of $660$ dollars at the end of the '\n",
      "                      '20-day period. He earns $60$ dollars per day if he '\n",
      "                      'works, and loses $30$ dollars per day if he does not '\n",
      "                      \"work. Let's denote the number of days Sam worked as $w$ \"\n",
      "                      'and the number of days he did not work as $d$. We know '\n",
      "                      'that $w + d = 20$ (since he worked for 20 days in '\n",
      "                      'total).\\n'\n",
      "                      '\\n'\n",
      "                      \"Sam's total earnings can be expressed as:\\n\"\n",
      "                      '\\\\[60w - 30d = 660\\\\]\\n'\n",
      "                      '\\n'\n",
      "                      'We now have a system of two equations:\\n'\n",
      "                      '1. $w + d = 20$\\n'\n",
      "                      '2. $60w - 30d = 660$\\n'\n",
      "                      '\\n'\n",
      "                      'We can solve this system of equations to find the '\n",
      "                      'values of $w$ and $d$. First, we can solve the first '\n",
      "                      'equation for $w$:\\n'\n",
      "                      '\\\\[w = 20 - d\\\\]\\n'\n",
      "                      '\\n'\n",
      "                      'Substitute this expression for $w$ into the second '\n",
      "                      'equation:\\n'\n",
      "                      '\\\\[60(20 - d) - 30d = 660\\\\]\\n'\n",
      "                      '\\n'\n",
      "                      'Now, we can solve for $d$:\\n'\n",
      "                      '\\\\[1200 - '}]}\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "stats = compute_grpo_loss(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    example=math_train[4],\n",
    "    device=device,\n",
    "    num_rollouts=2,\n",
    "    max_new_tokens=256,\n",
    "    temperature=0.8,\n",
    "    top_p=0.9\n",
    ")\n",
    "pprint(stats)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a5c14dd",
   "metadata": {},
   "source": [
    "When all rewards are 0.0, so they are the advantages and so the loss. In this case, the weight eould not be updated at all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "f4ece9e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def train_rlvr_grpo(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    math_data,\n",
    "    device,\n",
    "    steps=None,\n",
    "    num_rollouts=2,\n",
    "    max_new_tokens=256,\n",
    "    temperature=0.8,\n",
    "    top_p=0.9,\n",
    "    lr=1e-5,\n",
    "    checkpoint_every=50,\n",
    "    checkpoint_dir=\".\",\n",
    "    csv_log_path=None,\n",
    "):\n",
    "    if steps is None:\n",
    "        steps = len(math_data)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "    current_step = 0\n",
    "    if csv_log_path is None:\n",
    "        timestamp = time.strftime(\"%Y%m%d_%H%M%S\")\n",
    "        csv_log_path = f\"train_rlvr_grpo_metrics_{timestamp}.csv\"\n",
    "    csv_log_path = Path(csv_log_path)\n",
    "    try:\n",
    "        for step in range(steps):\n",
    "            optimizer.zero_grad()\n",
    "            current_step = step + 1\n",
    "            example = math_data[step % len(math_data)]\n",
    "            stats = compute_grpo_loss(\n",
    "                model=model,\n",
    "                tokenizer=tokenizer,\n",
    "                example=example,\n",
    "                device=device,\n",
    "                num_rollouts=num_rollouts,\n",
    "                max_new_tokens=max_new_tokens,\n",
    "                temperature=temperature,\n",
    "                top_p=top_p,\n",
    "            )\n",
    "            stats[\"loss_tensor\"].backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "            reward_avg = torch.tensor(stats[\"rewards\"]).mean().item()\n",
    "            step_tokens = sum(\n",
    "                sample[\"gen_len\"] for sample in stats[\"samples\"]\n",
    "            )\n",
    "            avg_response_len = (\n",
    "                step_tokens / len(stats[\"samples\"]) if stats[\"samples\"] else 0.0\n",
    "            )\n",
    "            append_csv_metrics(\n",
    "                csv_log_path, current_step, steps,\n",
    "                stats[\"loss\"], reward_avg, avg_response_len,\n",
    "            )\n",
    "            print(\n",
    "                f\"[Step {current_step}/{steps}] \"\n",
    "                f\"loss={stats['loss']:.4f} \"\n",
    "                f\"reward_avg={reward_avg:.3f} \"\n",
    "                f\"avg_resp_len={avg_response_len:.1f}\"\n",
    "            )\n",
    "            if checkpoint_every and current_step % checkpoint_every == 0:\n",
    "                ckpt_path = save_checkpoint(\n",
    "                    model=model,\n",
    "                    checkpoint_dir=checkpoint_dir,\n",
    "                    step=current_step,\n",
    "                )\n",
    "                print(f\"Saved checkpoint to {ckpt_path}\")\n",
    "    except KeyboardInterrupt:\n",
    "        ckpt_path = save_checkpoint(\n",
    "            model=model,\n",
    "            checkpoint_dir=checkpoint_dir,\n",
    "            step=max(1, current_step),\n",
    "            suffix=\"interrupt\",\n",
    "        )\n",
    "        print(f\"\\nKeyboardInterrupt. Saved checkpoint to {ckpt_path}\")\n",
    "        return model\n",
    "    return model\n",
    "\n",
    "def save_checkpoint(model, checkpoint_dir, step, suffix=\"\"):\n",
    "    checkpoint_dir = Path(checkpoint_dir)\n",
    "    checkpoint_dir.mkdir(parents=True, exist_ok=True)\n",
    "    suffix = f\"-{suffix}\" if suffix else \"\"\n",
    "    ckpt_path = (\n",
    "        checkpoint_dir /\n",
    "        f\"qwen3-0.6B-rlvr-grpo-step{step:05d}{suffix}.pth\"\n",
    "    )\n",
    "    torch.save(model.state_dict(), ckpt_path)\n",
    "    return ckpt_path\n",
    "\n",
    "def append_csv_metrics(\n",
    "    csv_log_path, step_idx, total_steps,\n",
    "    loss, reward_avg, avg_response_len,\n",
    "):\n",
    "    if not csv_log_path.exists():\n",
    "        csv_log_path.write_text(\n",
    "            \"step,total_steps,loss,reward_avg,avg_response_len\\n\",\n",
    "            encoding=\"utf-8\",\n",
    "        )\n",
    "    with csv_log_path.open(\"a\", encoding=\"utf-8\") as f:\n",
    "        f.write(\n",
    "            f\"{step_idx},{total_steps},{loss:.6f},{reward_avg:.6f},\"\n",
    "            f\"{avg_response_len:.6f}\\n\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "4a227933",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 1/50] loss=-0.0000 reward_avg=0.000 avg_resp_len=5.5\n",
      "[Step 2/50] loss=-0.0000 reward_avg=0.000 avg_resp_len=6.8\n",
      "[Step 3/50] loss=-0.7200 reward_avg=0.375 avg_resp_len=8.2\n",
      "[Step 4/50] loss=2.8870 reward_avg=0.500 avg_resp_len=55.0\n",
      "[Step 5/50] loss=-0.0000 reward_avg=0.000 avg_resp_len=164.2\n",
      "Saved checkpoint to qwen3-0.6B-rlvr-grpo-step00005.pth\n",
      "[Step 6/50] loss=-0.0000 reward_avg=0.000 avg_resp_len=117.8\n",
      "\n",
      "KeyboardInterrupt. Saved checkpoint to qwen3-0.6B-rlvr-grpo-step00007-interrupt.pth\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Qwen3Model(\n",
       "  (tok_emb): Embedding(151936, 1024)\n",
       "  (trf_blocks): ModuleList(\n",
       "    (0-27): 28 x TransformerBlock(\n",
       "      (att): GroupedQueryAttention(\n",
       "        (W_query): Linear(in_features=1024, out_features=2048, bias=False)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "        (out_proj): Linear(in_features=2048, out_features=1024, bias=False)\n",
       "        (q_norm): RMSNorm()\n",
       "        (k_norm): RMSNorm()\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (fc1): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "        (fc2): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "        (fc3): Linear(in_features=3072, out_features=1024, bias=False)\n",
       "      )\n",
       "      (norm1): RMSNorm()\n",
       "      (norm2): RMSNorm()\n",
       "    )\n",
       "  )\n",
       "  (final_norm): RMSNorm()\n",
       "  (out_head): Linear(in_features=1024, out_features=151936, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(1)\n",
    "train_rlvr_grpo(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    math_data=math_train,\n",
    "    device=device,\n",
    "    steps=50,\n",
    "    num_rollouts=4,\n",
    "    max_new_tokens=512,\n",
    "    temperature=0.8,\n",
    "    top_p=0.9,\n",
    "    lr=1e-5,\n",
    "    checkpoint_every=5,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d1d8554",
   "metadata": {},
   "source": [
    "Overall the loss fluctuates a lot, and thius is normal in RL training. Long term we want to see:\n",
    "\n",
    "1) **REWARD AVG** to increase (model produces accurate responses)\n",
    "2) The **REASONING ACCURACY** should improve (we evaluate this later)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "852f1416",
   "metadata": {},
   "source": [
    "# Loading and Evaluating saved model checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "06024bf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "qwen3-0.6B-rlvr-grpo-step00050.pth: 100% (1433 MiB / 1433 MiB)\n"
     ]
    }
   ],
   "source": [
    "from reasoning_from_scratch.qwen3 import download_qwen3_grpo_checkpoints\n",
    "download_qwen3_grpo_checkpoints(grpo_type=\"no_kl\", step=\"00050\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b9c3bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python  scripts/evaluate_math500.py --dataset_size 500 --which_model base --checkpoint_path qwen3-0.6B-rlvr-grpo-step00050.pth"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
