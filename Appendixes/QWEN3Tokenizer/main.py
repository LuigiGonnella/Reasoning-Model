import re
from tokenizers import Tokenizer
from pathlib import Path

class Qwen3Tokenizer:
    _SPECIALS = [
    "<|endoftext|>",
    "<|im_start|>", "<|im_end|>",
    "<|object_ref_start|>", "<|object_ref_end|>",
    "<|box_start|>", "<|box_end|>",
    "<|quad_start|>", "<|quad_end|>",
    "<|vision_start|>", "<|vision_end|>",
    "<|vision_pad|>", "<|image_pad|>", "<|video_pad|>",
    ]
    _SPLIT_RE = re.compile(r"(<\|[^>]+?\|>)")
    def __init__(self,
        tokenizer_file_path="tokenizer-base.json",
        apply_chat_template=False,
        add_generation_prompt=False,
        add_thinking=False):
        self.apply_chat_template = apply_chat_template
        self.add_generation_prompt = add_generation_prompt
        self.add_thinking = add_thinking
        tok_path = Path(tokenizer_file_path)

        if not tok_path.is_file():
            raise FileNotFoundError(
            f"Tokenizer file '{tok_path}' not found. "
            )
        
        self._tok = Tokenizer.from_file(str(tok_path))
        self._special_to_id = {t: self._tok.token_to_id(t)
            for t in self._SPECIALS}
        
        self.pad_token = "<|endoftext|>"
        self.pad_token_id = self._special_to_id.get(self.pad_token)
        f = tok_path.name.lower() 

        if "base" in f and "reasoning" not in f: 
            self.eos_token = "<|endoftext|>" 
        else: 
            self.eos_token = "<|im_end|>" 
        
        self.eos_token_id = self._special_to_id.get(self.eos_token)
        
    def encode(self, prompt, chat_wrapped=None):
        if chat_wrapped is None:
            chat_wrapped = self.apply_chat_template

        stripped = prompt.strip()

        if stripped in self._special_to_id and "\n" not in stripped:
            return [self._special_to_id[stripped]]
        
        if chat_wrapped:
            prompt = self._wrap_chat(prompt)

        ids = []
        for part in filter(None, self._SPLIT_RE.split(prompt)):
            if part in self._special_to_id:
                ids.append(self._special_to_id[part])
            else:
                ids.extend(self._tok.encode(part).ids)
        return ids
    
    def decode(self, token_ids):
        return self._tok.decode(token_ids, skip_special_tokens=False)
    
    def _wrap_chat(self, user_msg):
        s = f"<|im_start|>user\n{user_msg}<|im_end|>\n"
        if self.add_generation_prompt:
            s += "<|im_start|>assistant"
            if self.add_thinking:
                s += "\n" #B
            else:
                s += "\n<think>\n\n</think>\n\n"
        return s
