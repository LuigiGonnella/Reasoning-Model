{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "15ef20b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import torch\n",
    "from reasoning_from_scratch.ch02 import (\n",
    "        get_device\n",
    ")\n",
    "from reasoning_from_scratch.qwen3 import (\n",
    "        download_qwen3_small,\n",
    "        Qwen3Tokenizer,\n",
    "        Qwen3Model,\n",
    "        QWEN_CONFIG_06_B\n",
    ")\n",
    "from reasoning_from_scratch.qwen3 import KVCache\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "07b39c3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model_and_tokenizer(which_model, device, use_compile, local_dir=\"qwen3\"):\n",
    "\n",
    "    if which_model == \"base\":\n",
    "        download_qwen3_small(kind=\"base\", tokenizer_only=False, out_dir=local_dir)\n",
    "        tokenizer_path = Path(local_dir) / \"tokenizer-base.json\"\n",
    "        model_path = Path(local_dir) / \"qwen3-0.6B-base.pth\"\n",
    "        tokenizer = Qwen3Tokenizer(tokenizer_file_path=tokenizer_path)\n",
    "\n",
    "    elif which_model == \"reasoning\":\n",
    "        download_qwen3_small(kind=\"reasoning\", tokenizer_only=False, out_dir=local_dir)\n",
    "        tokenizer_path = Path(local_dir) / \"tokenizer-reasoning.json\"\n",
    "        model_path = Path(local_dir) / \"qwen3-0.6B-reasoning.pth\"\n",
    "        tokenizer = Qwen3Tokenizer(\n",
    "                tokenizer_file_path=tokenizer_path,\n",
    "                apply_chat_template=True,\n",
    "                add_generation_prompt=True,\n",
    "                add_thinking=True,\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(f\"Invalid choice: which_model={which_model}\")\n",
    "    \n",
    "    model = Qwen3Model(QWEN_CONFIG_06_B)\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "    model.to(device)\n",
    "    if use_compile: #Optionally set to true to enable model compilation\n",
    "        torch._dynamo.config.allow_unspec_int_on_nn_module = True\n",
    "        model = torch.compile(model)\n",
    "    return model, tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b0aae7af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using NVIDIA CUDA GPU\n",
      "✓ qwen3\\qwen3-0.6B-base.pth already up-to-date\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Utente\\AppData\\Local\\Temp\\ipykernel_719604\\2595948078.py:23: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(model_path))\n"
     ]
    }
   ],
   "source": [
    "WHICH_MODEL = \"base\" #Uses the base model, similar to chapter 2, by default\n",
    "device = get_device()\n",
    "model, tokenizer = load_model_and_tokenizer(\n",
    "        which_model=WHICH_MODEL,\n",
    "        device=device,\n",
    "        use_compile=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "31a7bde6",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.inference_mode()\n",
    "def generate_text_basic_stream_cache(model, input_ids, max_new_tokens, eos_token_id=None):\n",
    "    model.eval()\n",
    "\n",
    "    cache = KVCache(n_layers=model.cfg['n_layers'])\n",
    "    model.reset_kv_cache()\n",
    "\n",
    "    out = model(input_ids, cache=cache)[:, -1]\n",
    "\n",
    "    for _ in range(max_new_tokens):\n",
    "        next_token = torch.argmax(out, dim=-1, keepdim=True)\n",
    "\n",
    "        if (eos_token_id is not None\n",
    "                and next_token.item() == eos_token_id):\n",
    "            break\n",
    "\n",
    "        yield next_token  # Yield each token as it's generated\n",
    "\n",
    "        out = model(next_token, cache=cache)[:, -1]\n",
    "        \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "faf69eca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " To find the value of \\( a^2 + b^2 \\) given that \\( a + b = 3 \\) and \\( ab = \\frac{13}{6} \\), we can use the following algebraic identity:\n",
      "\n",
      "\\[\n",
      "a^2 + b^2 = (a + b)^2 - 2ab\n",
      "\\]\n",
      "\n",
      "**Step 1:** Substitute the given values into the equation.\n",
      "\n",
      "\\[\n",
      "a^2 + b^2 = (3)^2 - 2 \\left( \\frac{13}{6} \\right)\n",
      "\\]\n",
      "\n",
      "**Step 2:** Calculate \\( (3)^2 \\).\n",
      "\n",
      "\\[\n",
      "(3)^2 = 9\n",
      "\\]\n",
      "\n",
      "**Step 3:** Calculate \\( 2 \\times \\frac{13}{6} \\).\n",
      "\n",
      "\\[\n",
      "2 \\times \\frac{13}{6} = \\frac{26}{6} = \\frac{13}{3}\n",
      "\\]\n",
      "\n",
      "**Step 4:** Subtract the second result from the first.\n",
      "\n",
      "\\[\n",
      "a^2 + b^2 = 9 - \\frac{13}{3}\n",
      "\\]\n",
      "\n",
      "**Step 5:** Convert 9 to a fraction with a denominator of 3 to perform the subtraction.\n",
      "\n",
      "\\[\n",
      "9 = \\frac{27}{3}\n",
      "\\]\n",
      "\n",
      "\\[\n",
      "a^2 + b^2 = \\frac{27}{3} - \\frac{13}{3} = \\frac{14}{3}\n",
      "\\]\n",
      "\n",
      "**Final Answer:**\n",
      "\n",
      "\\[\n",
      "\\boxed{\\dfrac{14}{3}}\n",
      "\\]"
     ]
    }
   ],
   "source": [
    "prompt = ( #MATH PROBLEM\n",
    "    r\"If $a+b=3$ and $ab=\\tfrac{13}{6}$, \"\n",
    "    r\"what is the value of $a^2+b^2$?\"\n",
    ")\n",
    "\n",
    "input_ids = torch.tensor(\n",
    "    tokenizer.encode(prompt),\n",
    "    device=device\n",
    ").unsqueeze(0)\n",
    "\n",
    "all_token_ids = []\n",
    "\n",
    "for token in generate_text_basic_stream_cache(model, input_ids, max_new_tokens=2048, eos_token_id=tokenizer.eos_token_id):\n",
    "    token_id = token.squeeze(0)\n",
    "    decoded_id = tokenizer.decode([token_id])\n",
    "    print(\n",
    "        decoded_id,\n",
    "        end='',\n",
    "        flush=True\n",
    "    )\n",
    "\n",
    "    all_token_ids.append(token_id)\n",
    "\n",
    "all_tokens = tokenizer.decode(all_token_ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7a3f21db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       " To find the value of \\( a^2 + b^2 \\) given that \\( a + b = 3 \\) and \\( ab = \\frac{13}{6} \\), we can use the following algebraic identity:\n",
       "\n",
       "\\[\n",
       "a^2 + b^2 = (a + b)^2 - 2ab\n",
       "\\]\n",
       "\n",
       "**Step 1:** Substitute the given values into the equation.\n",
       "\n",
       "\\[\n",
       "a^2 + b^2 = (3)^2 - 2 \\left( \\frac{13}{6} \\right)\n",
       "\\]\n",
       "\n",
       "**Step 2:** Calculate \\( (3)^2 \\).\n",
       "\n",
       "\\[\n",
       "(3)^2 = 9\n",
       "\\]\n",
       "\n",
       "**Step 3:** Calculate \\( 2 \\times \\frac{13}{6} \\).\n",
       "\n",
       "\\[\n",
       "2 \\times \\frac{13}{6} = \\frac{26}{6} = \\frac{13}{3}\n",
       "\\]\n",
       "\n",
       "**Step 4:** Subtract the second result from the first.\n",
       "\n",
       "\\[\n",
       "a^2 + b^2 = 9 - \\frac{13}{3}\n",
       "\\]\n",
       "\n",
       "**Step 5:** Convert 9 to a fraction with a denominator of 3 to perform the subtraction.\n",
       "\n",
       "\\[\n",
       "9 = \\frac{27}{3}\n",
       "\\]\n",
       "\n",
       "\\[\n",
       "a^2 + b^2 = \\frac{27}{3} - \\frac{13}{3} = \\frac{14}{3}\n",
       "\\]\n",
       "\n",
       "**Final Answer:**\n",
       "\n",
       "\\[\n",
       "\\boxed{\\dfrac{14}{3}}\n",
       "\\]"
      ],
      "text/plain": [
       "<IPython.core.display.Latex object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Latex, display\n",
    "display(Latex(all_tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a47ee8a1",
   "metadata": {},
   "source": [
    "Now we can prepare the wrapper function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0dffb96f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text_stream_concat(model, tokenizer, prompt, device, max_new_tokens, verbose=False):\n",
    "\n",
    "    input_ids = torch.tensor(\n",
    "        tokenizer.encode(prompt),\n",
    "        device=device\n",
    "    ).unsqueeze(0)\n",
    "\n",
    "    all_token_ids = []\n",
    "\n",
    "    for token in generate_text_basic_stream_cache(model, input_ids, max_new_tokens=max_new_tokens, eos_token_id=tokenizer.eos_token_id):\n",
    "        token_id = token.squeeze(0)\n",
    "        all_token_ids.append(token_id)\n",
    "        \n",
    "\n",
    "        if verbose:\n",
    "            decoded_id = tokenizer.decode([token_id])\n",
    "            print(\n",
    "                decoded_id,\n",
    "                end='',\n",
    "                flush=True\n",
    "            )\n",
    "\n",
    "    return tokenizer.decode(all_token_ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a92341b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " To find the value of \\( a^2 + b^2 \\) given that \\( a + b = 3 \\) and \\( ab = \\frac{13}{6} \\), we can use the following algebraic identity:\n",
      "\n",
      "\\[\n",
      "a^2 + b^2 = (a + b)^2 - 2ab\n",
      "\\]\n",
      "\n",
      "**Step 1:** Substitute the given values into the equation.\n",
      "\n",
      "\\[\n",
      "a^2 + b^2 = (3)^2 - 2 \\left( \\frac{13}{6} \\right)\n",
      "\\]\n",
      "\n",
      "**Step 2:** Calculate \\( (3)^2 \\).\n",
      "\n",
      "\\[\n",
      "(3)^2 = 9\n",
      "\\]\n",
      "\n",
      "**Step 3:** Calculate \\( 2 \\times \\frac{13}{6} \\).\n",
      "\n",
      "\\[\n",
      "2 \\times \\frac{13}{6} = \\frac{26}{6} = \\frac{13}{3}\n",
      "\\]\n",
      "\n",
      "**Step 4:** Subtract the second result from the first.\n",
      "\n",
      "\\[\n",
      "a^2 + b^2 = 9 - \\frac{13}{3}\n",
      "\\]\n",
      "\n",
      "**Step 5:** Convert 9 to a fraction with a denominator of 3 to perform the subtraction.\n",
      "\n",
      "\\[\n",
      "9 = \\frac{27}{3}\n",
      "\\]\n",
      "\n",
      "\\[\n",
      "a^2 + b^2 = \\frac{27}{3} - \\frac{13}{3} = \\frac{14}{3}\n",
      "\\]\n",
      "\n",
      "**Final Answer:**\n",
      "\n",
      "\\[\n",
      "\\boxed{\\dfrac{14}{3}}\n",
      "\\]"
     ]
    }
   ],
   "source": [
    "model_answer = generate_text_stream_concat(model, tokenizer, prompt, device, max_new_tokens=2048, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "677de1e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_last_boxed(text):\n",
    "    boxed_start_idx = text.rfind(r\"\\boxed\") \n",
    "    if boxed_start_idx == -1:\n",
    "        return None\n",
    "    current_idx = boxed_start_idx + len(r\"\\boxed\") \n",
    "\n",
    "    while current_idx < len(text) and text[current_idx].isspace():\n",
    "        current_idx += 1\n",
    "    \n",
    "    if current_idx >= len(text) or text[current_idx] != \"{\":\n",
    "        return None\n",
    "    \n",
    "    current_idx += 1\n",
    "    brace_depth = 1\n",
    "    content_start_idx = current_idx\n",
    "    \n",
    "    while current_idx < len(text) and brace_depth > 0:\n",
    "        char = text[current_idx]\n",
    "        if char == \"{\":\n",
    "            brace_depth += 1\n",
    "        elif char == \"}\":\n",
    "            brace_depth -= 1\n",
    "        current_idx += 1\n",
    "\n",
    "    if brace_depth != 0: \n",
    "        return None\n",
    "    \n",
    "    return text[content_start_idx:current_idx-1] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4c11632b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle \\dfrac{14}{3}$"
      ],
      "text/plain": [
       "<IPython.core.display.Math object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Math\n",
    "\n",
    "extracted_answer = get_last_boxed(model_answer)\n",
    "display(Math(extracted_answer))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0346137e",
   "metadata": {},
   "source": [
    "We can also handle cases in which the model fails to format a correct answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9ffec0de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "RE_NUMBER = re.compile( \n",
    "        r\"-?(?:\\d+/\\d+|\\d+(?:\\.\\d+)?(?:[eE][+-]?\\d+)?)\"\n",
    "    )\n",
    "def extract_final_candidate(text, fallback=\"number_then_full\"):\n",
    "    result = \"\" \n",
    "\n",
    "    if text: \n",
    "        boxed = get_last_boxed(text.strip())\n",
    "\n",
    "        if boxed:\n",
    "            result = boxed.strip().strip(\"$ \")\n",
    "    \n",
    "        elif fallback in (\"number_then_full\", \"number_only\"):\n",
    "            m = RE_NUMBER.findall(text)\n",
    "\n",
    "            if m:\n",
    "                result = m[-1] \n",
    "            elif fallback == \"number_then_full\":\n",
    "                result = text \n",
    "\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5e0962d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle 14/3.$"
      ],
      "text/plain": [
       "<IPython.core.display.Math object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "final_candidate = extract_final_candidate(r\"\\boxed{ 14/3. }\")\n",
    "display(Math(final_candidate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7c37a411",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle 14/3$"
      ],
      "text/plain": [
       "<IPython.core.display.Math object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "final_candidate = extract_final_candidate(\"abc < > 14/3 abc\") #last number is 14/3\n",
    "display(Math(final_candidate))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c1eec20",
   "metadata": {},
   "source": [
    "Now we can NORMALIZE the extracted answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e372e9d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "LATEX_FIXES = [  # A\n",
    "    (r\"\\\\left\\s*\", \"\"),\n",
    "    (r\"\\\\right\\s*\", \"\"),\n",
    "    (r\"\\\\,|\\\\!|\\\\;|\\\\:\", \"\"),\n",
    "    (r\"\\\\cdot\", \"*\"),\n",
    "    (r\"\\u00B7|\\u00D7\", \"*\"),\n",
    "    (r\"\\\\\\^\\\\circ\", \"\"),\n",
    "    (r\"\\\\dfrac\", r\"\\\\frac\"),\n",
    "    (r\"\\\\tfrac\", r\"\\\\frac\"),\n",
    "    (r\"°\", \"\"),\n",
    "]\n",
    "RE_SPECIAL = re.compile(r\"<\\|[^>]+?\\|>\")  # B\n",
    "SUPERSCRIPT_MAP = {\n",
    "    \"⁰\": \"0\", \"¹\": \"1\", \"²\": \"2\", \"³\": \"3\", \"⁴\": \"4\",  # C\n",
    "    \"⁵\": \"5\", \"⁶\": \"6\", \"⁷\": \"7\", \"⁸\": \"8\", \"⁹\": \"9\",  # C\n",
    "    \"⁺\": \"+\", \"⁻\": \"-\", \"⁽\": \"(\", \"⁾\": \")\",  # C\n",
    "}\n",
    "\n",
    "def normalize_text(text):\n",
    "    if not text:\n",
    "        return \"\"\n",
    "    text = RE_SPECIAL.sub(\"\", text).strip()\n",
    "    # D\n",
    "    match = re.match(r\"^[A-Za-z]\\s*[.:]\\s*(.+)$\", text)\n",
    "    if match:\n",
    "        text = match.group(1)\n",
    "    text = re.sub(r\"\\^\\s*\\{\\s*\\\\circ\\s*\\}\", \"\", text)  # D\n",
    "    text = re.sub(r\"\\^\\s*\\\\circ\", \"\", text)  # E\n",
    "    text = text.replace(\"°\", \"\")  # E\n",
    "    match = re.match(r\"^\\\\text\\{(?P<x>.+?)\\}$\", text)  # F\n",
    "    if match:\n",
    "        text = match.group(\"x\")\n",
    "    text = re.sub(r\"\\\\\\(|\\\\\\)|\\\\\\[|\\\\\\]\", \"\", text)  # G\n",
    "    for pat, rep in LATEX_FIXES:  # H\n",
    "        text = re.sub(pat, rep, text)\n",
    "    \n",
    "    def convert_superscripts(s, base=None):\n",
    "        converted = \"\".join(\n",
    "            SUPERSCRIPT_MAP[ch] if ch in SUPERSCRIPT_MAP else ch\n",
    "            for ch in s\n",
    "        )\n",
    "        if base is None:\n",
    "            return converted\n",
    "        return f\"{base}**{converted}\"\n",
    "    \n",
    "    text = re.sub(\n",
    "        r\"([0-9A-Za-z\\)\\]\\}])([⁰¹²³⁴⁵⁶⁷⁸⁹⁺⁻]+)\",\n",
    "        lambda m: convert_superscripts(m.group(2), base=m.group(1)),\n",
    "        text,\n",
    "    )\n",
    "    text = convert_superscripts(text)\n",
    "    # I\n",
    "    text = text.replace(\"\\\\%\", \"%\").replace(\"$\", \"\").replace(\"%\", \"\")\n",
    "    text = re.sub(\n",
    "        r\"\\\\sqrt\\s*\\{([^}]*)\\}\",\n",
    "        lambda match: f\"sqrt({match.group(1)})\",\n",
    "        text,\n",
    "    )\n",
    "    text = re.sub(\n",
    "        r\"\\\\sqrt\\s+([^\\\\\\s{}]+)\",\n",
    "        lambda match: f\"sqrt({match.group(1)})\",\n",
    "        text,\n",
    "    )\n",
    "    # J\n",
    "    text = re.sub(\n",
    "        r\"\\\\frac\\s*\\{([^{}]+)\\}\\s*\\{([^{}]+)\\}\",\n",
    "        lambda match: f\"({match.group(1)})/({match.group(2)})\",\n",
    "        text,\n",
    "    )\n",
    "    text = re.sub(\n",
    "        r\"\\\\frac\\s+([^\\s{}]+)\\s+([^\\s{}]+)\",\n",
    "        lambda match: f\"({match.group(1)})/({match.group(2)})\",\n",
    "        text,\n",
    "    )\n",
    "    # K\n",
    "    text = text.replace(\"^\", \"**\")\n",
    "    text = re.sub(\n",
    "        r\"(?<=\\d)\\s+(\\d+/\\d+)\",\n",
    "        lambda match: \"+\" + match.group(1),\n",
    "        text,\n",
    "    )\n",
    "    # L\n",
    "    text = re.sub(\n",
    "        r\"(?<=\\d),(?=\\d\\d\\d(\\D|$))\",\n",
    "        \"\",\n",
    "        text,\n",
    "    )\n",
    "    return text.replace(\"{\", \"\").replace(\"}\", \"\").strip().lower()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3ac40f8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14)/(3)\n"
     ]
    }
   ],
   "source": [
    "print(normalize_text(extract_final_candidate(model_answer)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "79a8226b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14)/(3)\n"
     ]
    }
   ],
   "source": [
    "print(normalize_text(r\"\\text{\\[\\frac{14}{3}\\]}\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e47d28a",
   "metadata": {},
   "source": [
    "We can now verify the mathematical equivalence between the extracted answer from LLm and a GT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b9307ca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sympy.parsing import sympy_parser as spp\n",
    "from sympy.core.sympify import SympifyError\n",
    "from sympy.polys.polyerrors import PolynomialError\n",
    "from tokenize import TokenError\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "11c79e60",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sympy_parser(expr):\n",
    "    try:\n",
    "        return spp.parse_expr(expr, transformations=(\n",
    "\n",
    "            *spp.standard_transformations, #like handling parenthesis invariant symbols\n",
    "            spp.implicit_multiplication_application, #allow omitted mul symbols, like 2y == 2*y\n",
    "\n",
    "            ),\n",
    "        evaluate=True,\n",
    "        )\n",
    "    except (SympifyError, SyntaxError, TypeError, AttributeError,\n",
    "            IndexError, TokenError, ValueError, PolynomialError):\n",
    "        return None\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "56c2540a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14/3\n"
     ]
    }
   ],
   "source": [
    "print(sympy_parser(\n",
    "    normalize_text(\n",
    "        extract_final_candidate(\n",
    "            model_answer))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9e9ffc5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14/3\n"
     ]
    }
   ],
   "source": [
    "print(sympy_parser(\"28/6\")) #normalized by sympy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc57c495",
   "metadata": {},
   "source": [
    "We can noe build the equality function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "aa605c37",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sympy import simplify\n",
    "\n",
    "def equality_check(expr_gt, expr_pred):\n",
    "\n",
    "    if expr_gt == expr_pred:\n",
    "        return True\n",
    "    \n",
    "    gt, pred = sympy_parser(expr_gt), sympy_parser(expr_pred)\n",
    "\n",
    "    if gt is not None and pred is not None:\n",
    "\n",
    "        try:\n",
    "            return simplify(gt - pred) == 0 #for example, 14/3 and 28/6\n",
    "        \n",
    "        except(SympifyError, TypeError):\n",
    "            return False\n",
    "    \n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "00751261",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(equality_check(\n",
    "    normalize_text(\"13/4.\"),\n",
    "    normalize_text(r\"(13)/(4)\")\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "278d9685",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(equality_check(\n",
    "    normalize_text(\"0.5\"),\n",
    "    normalize_text(r\"(1)/(2)\")\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "459a20a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "print(equality_check(\n",
    "    normalize_text(\"14/3\"),\n",
    "    normalize_text(\"15/3\")\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c50b28eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "print(equality_check(\n",
    "    normalize_text(\"(14/3, 2/3)\"),\n",
    "    normalize_text(\"(14/3, 4/6)\")\n",
    "))\n",
    "#it cannot handle tuples as of now"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e23cd186",
   "metadata": {},
   "source": [
    "Now we implement an helper function that can handle the tuple-like expressions by isolating each term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "87bf91a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_into_parts(text):\n",
    "    result = [text]\n",
    "    if (text):\n",
    "\n",
    "        if (\n",
    "            len(text) >= 2 and\n",
    "            text[0] in \"([\" and text[-1] in \")]\"\n",
    "            and \",\" in text[1:-1]\n",
    "        ):\n",
    "            items = [p.strip() for p in text[1:-1].split(\",\")]\n",
    "            if all(items): #all not None\n",
    "                result = items\n",
    "    else:\n",
    "        result = []\n",
    "    \n",
    "    return result        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "45d575eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['14/3', '2/3']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_into_parts(normalize_text(r\"(14/3, 2/3)\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a3b0301",
   "metadata": {},
   "source": [
    "Now we can implement a function that grades the result w.r.t. the GT that generalize the equality_check function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6d4a027b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grade_answer(pred_text, gt_text) -> bool: #or True or False\n",
    "    result = False\n",
    "    if pred_text is not None and gt_text is not None:\n",
    "        gt_parts = split_into_parts(normalize_text(gt_text))\n",
    "        pred_parts = split_into_parts(normalize_text(pred_text))\n",
    "\n",
    "        if (gt_parts and pred_parts \n",
    "            and len(gt_parts) == len(pred_parts)):\n",
    "            result = all(equality_check(gt, pred) for gt, pred in zip(gt_parts, pred_parts))\n",
    "    \n",
    "    return result\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "cef8a42d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grade_answer(\"14/3\", r\"\\frac{14}{3}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "857a5969",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#with TUPLES\n",
    "grade_answer(r\"(14/3, 2/3)\", \"(14/3, 4/6)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6427ed1e",
   "metadata": {},
   "source": [
    "We can check with more tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e962c91d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tests = [ #A\n",
    "(\"check_1\", \"3/4\", r\"\\frac{3}{4}\", True),\n",
    "(\"check_2\", \"(3)/(4)\", r\"3/4\", True),\n",
    "(\"check_3\", r\"\\frac{\\sqrt{8}}{2}\", \"sqrt(2)\", True),\n",
    "(\"check_4\", r\"\\( \\frac{1}{2} + \\frac{1}{6} \\)\", \"2/3\", True),\n",
    "(\"check_5\", \"(1, 2)\", r\"(1,2)\", True),\n",
    "(\"check_6\", \"(2, 1)\", \"(1, 2)\", False),\n",
    "(\"check_7\", \"(1, 2, 3)\", \"(1, 2)\", False),\n",
    "(\"check_8\", \"0.5\", \"1/2\", True),\n",
    "(\"check_9\", \"0.3333333333\", \"1/3\", False),\n",
    "(\"check_10\", \"1,234/2\", \"617\", True),\n",
    "(\"check_11\", r\"\\text{2/3}\", \"2/3\", True),\n",
    "(\"check_12\", \"50%\", \"1/2\", False),\n",
    "(\"check_13\", r\"2\\cdot 3/4\", \"3/2\", True),\n",
    "(\"check_14\", r\"90^\\circ\", \"90\", True),\n",
    "(\"check_15\", r\"\\left(\\frac{3}{4}\\right)\", \"3/4\", True),\n",
    "(\"check_16\", r\"2²\", \"2**2\", True),\n",
    "]\n",
    "\n",
    "def run_demos_table(tests):\n",
    "    header = (\"Test\", \"Expect\", \"Got\", \"Status\")\n",
    "    rows = []\n",
    "    for name, pred, gtruth, expect in tests:\n",
    "        got = grade_answer(pred, gtruth) #B\n",
    "        status = \"PASS\" if got == expect else \"FAIL\"\n",
    "        rows.append((name, str(expect), str(got), status))\n",
    "\n",
    "    data = [header] + rows\n",
    "    col_widths = [ #C\n",
    "        max(len(row[i]) for row in data)\n",
    "        for i in range(len(header))\n",
    "    ]\n",
    "\n",
    "    for row in data: #D\n",
    "        line = \" | \".join(\n",
    "            row[i].ljust(col_widths[i])\n",
    "            for i in range(len(header))\n",
    "        )\n",
    "\n",
    "        print(line)\n",
    "    passed = sum(r[3] == \"PASS\" for r in rows) #E\n",
    "    print(f\"\\nPassed {passed}/{len(rows)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8a0f0c46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test     | Expect | Got   | Status\n",
      "check_1  | True   | True  | PASS  \n",
      "check_2  | True   | True  | PASS  \n",
      "check_3  | True   | True  | PASS  \n",
      "check_4  | True   | True  | PASS  \n",
      "check_5  | True   | True  | PASS  \n",
      "check_6  | False  | False | PASS  \n",
      "check_7  | False  | False | PASS  \n",
      "check_8  | True   | True  | PASS  \n",
      "check_9  | False  | False | PASS  \n",
      "check_10 | True   | True  | PASS  \n",
      "check_11 | True   | True  | PASS  \n",
      "check_12 | False  | False | PASS  \n",
      "check_13 | True   | True  | PASS  \n",
      "check_14 | True   | True  | PASS  \n",
      "check_15 | True   | True  | PASS  \n",
      "check_16 | True   | True  | PASS  \n",
      "\n",
      "Passed 16/16\n"
     ]
    }
   ],
   "source": [
    "run_demos_table(tests)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6302c34e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test     | Expect | Got   | Status\n",
      "check_17 | True   | True  | PASS  \n",
      "check_18 | True   | True  | PASS  \n",
      "check_19 | True   | True  | PASS  \n",
      "check_20 | True   | False | FAIL  \n",
      "\n",
      "Passed 3/4\n"
     ]
    }
   ],
   "source": [
    "more_tests = [\n",
    "    # Different bracket types\n",
    "    (\"check_17\", \"[1, 2]\", \"(1, 2)\", True),\n",
    "\n",
    "    # Scientific notation\n",
    "    (\"check_18\", \"1e-3\", \"0.001\", True),\n",
    "\n",
    "    # Algebraic simplification with caret exponent\n",
    "    (\"check_19\", \"(-3)^2\", \"9\", True),\n",
    "\n",
    "    # Unicode minus (U+2212) vs ASCII hyphen-minus\n",
    "    (\"check_20\", \"−1\", \"-1\", True), \n",
    "\n",
    "]\n",
    "\n",
    "run_demos_table(more_tests)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7114612",
   "metadata": {},
   "source": [
    "Our function doesn't handle ASCII special characters, to address this, we could generalize the normalize_text function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1e8e7cb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test     | Expect | Got  | Status\n",
      "check_21 | True   | True | PASS  \n",
      "\n",
      "Passed 1/1\n"
     ]
    }
   ],
   "source": [
    "extra_tests_2 = [\n",
    "    ('check_21', extract_final_candidate('Text around 3 I think'), '3', True)\n",
    "]\n",
    "run_demos_table(extra_tests_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0631305",
   "metadata": {},
   "source": [
    "# Loading the evaluation dataset\n",
    "\n",
    "We will use the MATH-500 dataset (https://huggingface.co/datasets/HuggingFaceH4/MATH-500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c1894906",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9931dd50",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_math500_test(local_path='math500_test.json', save_copy=True):\n",
    "    local_path = Path(local_path)\n",
    "    url = (\n",
    "        \"https://raw.githubusercontent.com/rasbt/reasoning-from-scratch/\"\n",
    "        \"main/ch03/01_main-chapter-code/math500_test.json\"\n",
    "    )\n",
    "\n",
    "    if local_path.exists():\n",
    "        with open(local_path, 'r', encoding='utf-8') as f:\n",
    "            data = json.load(f) \n",
    "    else:\n",
    "        r = requests.get(url, timeout=30)\n",
    "        r.raise_for_status\n",
    "        data=r.json()\n",
    "\n",
    "        if save_copy:\n",
    "            with open(local_path, 'w', encoding='utf-8') as f:\n",
    "                json.dump(data, f, indent=2)\n",
    "\n",
    "    return data \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2c371f90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of entries: 500\n"
     ]
    }
   ],
   "source": [
    "math_data = load_math500_test()\n",
    "print(\"Number of entries:\", len(math_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "91ead9aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#OPTIONALLY we could have done it with huggingface\n",
    "\n",
    "# from datasets import load_dataset\n",
    "# dset = load_dataset(\"HuggingFaceH4/MATH-500\", split=\"test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cb886cb",
   "metadata": {},
   "source": [
    "let's print the first entry of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d0556bf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e8dfe6f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'answer': '\\\\left( 3, \\\\frac{\\\\pi}{2} \\\\right)',\n",
      " 'level': 2,\n",
      " 'problem': 'Convert the point $(0,3)$ in rectangular coordinates to polar '\n",
      "            'coordinates.  Enter your answer in the form $(r,\\\\theta),$ where '\n",
      "            '$r > 0$ and $0 \\\\le \\\\theta < 2 \\\\pi.$',\n",
      " 'solution': 'We have that $r = \\\\sqrt{0^2 + 3^2} = 3.$  Also, if we draw the '\n",
      "             'line connecting the origin and $(0,3),$ this line makes an angle '\n",
      "             'of $\\\\frac{\\\\pi}{2}$ with the positive $x$-axis.\\n'\n",
      "             '\\n'\n",
      "             '[asy]\\n'\n",
      "             'unitsize(0.8 cm);\\n'\n",
      "             '\\n'\n",
      "             'draw((-0.5,0)--(3.5,0));\\n'\n",
      "             'draw((0,-0.5)--(0,3.5));\\n'\n",
      "             'draw(arc((0,0),3,0,90),red,Arrow(6));\\n'\n",
      "             '\\n'\n",
      "             'dot((0,3), red);\\n'\n",
      "             'label(\"$(0,3)$\", (0,3), W);\\n'\n",
      "             'dot((3,0), red);\\n'\n",
      "             '[/asy]\\n'\n",
      "             '\\n'\n",
      "             'Therefore, the polar coordinates are $\\\\boxed{\\\\left( 3, '\n",
      "             '\\\\frac{\\\\pi}{2} \\\\right)}.$',\n",
      " 'subject': 'Precalculus',\n",
      " 'unique_id': 'test/precalculus/807.json'}\n"
     ]
    }
   ],
   "source": [
    "pprint(math_data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad80051e",
   "metadata": {},
   "source": [
    "We need that the model outputs te final answer as a boxed string. To increase the likelihood that this happens, we can feed into the model a System Prompt that encourage this format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c678a08d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def render_prompt(prompt):\n",
    "    template = \"\"\" \n",
    "You are an helpfu and powerful assistant.\n",
    "Answer the question and write the final result on a new line as:\n",
    "\\\\boxed{{ANSWER}}\n",
    "\n",
    "Question:\n",
    "{PROMPT}\n",
    "\n",
    "Answer:\n",
    "    \"\"\".format(PROMPT=prompt)\n",
    "\n",
    "    return template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "71aa6954",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "You are an helpfu and powerful assistant.\n",
      "Answer the question and write the final result on a new line as:\n",
      "\\boxed{ANSWER}\n",
      "\n",
      "Question:\n",
      "If $a+b=3$ and $ab=\\tfrac{13}{6}$, what is the value of $a^2+b^2$?\n",
      "\n",
      "Answer:\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "prompt = (\n",
    "    r\"If $a+b=3$ and $ab=\\tfrac{13}{6}$, \"\n",
    "    r\"what is the value of $a^2+b^2$?\"\n",
    ")\n",
    "prompt_fmt = render_prompt(prompt)\n",
    "print(prompt_fmt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "a982bad8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \\boxed{13}"
     ]
    }
   ],
   "source": [
    "generated_text = generate_text_stream_concat(model, tokenizer, prompt_fmt, device, max_new_tokens=2048, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25dd27e8",
   "metadata": {},
   "source": [
    "As we can see, this prmpt encourages the model to outputs short anwers, prioritizing speed against correctness. In fact, without this formatting, the output of the mdoel was longer and slower but correct (**14/3**), while now it is quicker but wrong (**13**)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6a1bd0d",
   "metadata": {},
   "source": [
    "On Math-500 dataset, it was proven that just changing, in the previous prompt, the qeury indicator word from 'Qeustion' to 'Problem' increases ther BASE model performance of nearly 20%, while the REASONING model drops its performance of nearly 30%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "ebcc0b4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mini_eval_demo(model, tokenizer, device):\n",
    "    ex = {\n",
    "        'problem': 'Comput 1/2 + 1/6',\n",
    "        'answer': '2/3'\n",
    "    }\n",
    "    prompt = render_prompt(ex['problem'])\n",
    "    gen_text = generate_text_stream_concat(model, tokenizer, prompt, device, max_new_tokens=64, verbose=True)\n",
    "    pred_answer = extract_final_candidate(gen_text)\n",
    "    is_correct = grade_answer(pred_answer, ex['answer'])\n",
    "\n",
    "    print(f\"\\nDevice: {device}\")\n",
    "    print(f\"Prediction: {pred_answer}\")\n",
    "    print(f\"Ground truth: {ex['answer']}\")\n",
    "    print(f\"Correct: {is_correct}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "212267ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \\boxed{1/3}\n",
      "Device: cuda\n",
      "Prediction: 1/3\n",
      "Ground truth: 2/3\n",
      "Correct: False\n"
     ]
    }
   ],
   "source": [
    "mini_eval_demo(model, tokenizer, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fce3bcc3",
   "metadata": {},
   "source": [
    "End-to-end evaluation on math-500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "f61e4d15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "fdd1e33c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eta_progress_message(processed, total, start_time, show_eta=False, label=\"Progress\"):\n",
    "    progress = f'{label}: {processed}/{total}'\n",
    "\n",
    "    if not show_eta or processed <= 0:\n",
    "        return progress\n",
    "    \n",
    "    elapsed = time.time() - start_time\n",
    "    if elapsed <= 0 :\n",
    "        return progress\n",
    "    \n",
    "    remaining = max(total - processed, 0)\n",
    "\n",
    "    if processed:\n",
    "        avg_time = elapsed / processed\n",
    "        eta_seconds = avg_time * remaining\n",
    "    else:\n",
    "        eta_seconds = 0\n",
    "    \n",
    "    eta_seconds = max(round(eta_seconds), 0)\n",
    "    minutes, rem_seconds = divmod(eta_seconds, 60) \n",
    "    hours, minutes, = divmod(minutes, 60)\n",
    "    if hours:\n",
    "        eta = f\"{hours}h {minutes:02d}m {rem_seconds:02d}s\"\n",
    "    elif minutes:\n",
    "        eta = f\"{minutes:02d}m {rem_seconds:02d}s\"\n",
    "    else:\n",
    "        eta = f\"{rem_seconds:02d}s\"\n",
    "        \n",
    "    return f\"{progress} | ETA: {eta}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "9527330e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_math500_stream(model, model_type, tokenizer, device, math_data, out_path=None, max_new_tokens=512, verbose=False):\n",
    "    if out_path is None:\n",
    "        dev_name = str(device).replace(':', '-')\n",
    "        out_path = Path(f'math500-{dev_name}-{model_type}.jsonl') #jsonl is a file format in which we have a JSON entry for each row\n",
    "    \n",
    "    num_examples = len(math_data)\n",
    "    num_correct = 0\n",
    "    start_time = time.time()\n",
    "\n",
    "    with open(out_path, 'w', encoding=\"utf-8\") as f:\n",
    "        for i, row in enumerate(math_data, start=1):\n",
    "            prompt = render_prompt(row['problem'])\n",
    "            gen_text = generate_text_stream_concat(model, tokenizer, prompt, device, max_new_tokens=max_new_tokens, verbose=verbose)\n",
    "\n",
    "            extracted = extract_final_candidate(gen_text)\n",
    "            is_correct = grade_answer(extracted, row['answer'])\n",
    "\n",
    "            num_correct += int(is_correct)\n",
    "\n",
    "            record = { \n",
    "                \"index\": i,\n",
    "                \"problem\": row[\"problem\"],\n",
    "                \"gtruth_answer\": row[\"answer\"],\n",
    "                \"generated_text\": gen_text,\n",
    "                \"extracted\": extracted,\n",
    "                \"correct\": bool(is_correct),\n",
    "            }\n",
    "            f.write(json.dumps(record, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "            progress_message = eta_progress_message(processed=i, total=num_examples, start_time=start_time, show_eta=True, label='MATH-500')\n",
    "\n",
    "            print(progress_message, end=\"\\r\", flush=True)\n",
    "\n",
    "            if verbose: \n",
    "                print(\n",
    "                    f\"\\n\\n{'='*50}\\n{progress_message}\\n\"\n",
    "                    f\"{'='*50}\\nExtracted: {extracted}\\n\"\n",
    "                    f\"Expected: {row['answer']}\\n\"\n",
    "                    f\"Correct so far: {num_correct}\\n{'-'*50}\"\n",
    "                )\n",
    "\n",
    "    seconds_elapsed = time.time() - start_time\n",
    "    acc = num_correct / num_examples if num_examples else 0.0\n",
    "    print(f\"\\nAccuracy: {acc*100:.1f}% ({num_correct}/{num_examples})\")\n",
    "    print(f\"Total time: {seconds_elapsed/60:.1f} min\")\n",
    "    print(f\"Logs written to: {out_path}\")\n",
    "    return num_correct, num_examples, acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "a68dd770",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: base\n",
      "Device: cuda\n",
      "MATH-500: 10/10 | ETA: 00s\n",
      "Accuracy: 20.0% (2/10)\n",
      "Total time: 0.0 min\n",
      "Logs written to: math500-cuda-base.jsonl\n"
     ]
    }
   ],
   "source": [
    "print(\"Model:\", WHICH_MODEL)\n",
    "print(f'Device: {device}')\n",
    "num_correct, num_examples, acc = evaluate_math500_stream(\n",
    "    model, WHICH_MODEL, tokenizer, device,\n",
    "    math_data=math_data[:10], #only evaluates the first 10 examples\n",
    "    max_new_tokens=2048,\n",
    "    verbose=False \n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3842626a",
   "metadata": {},
   "source": [
    "The poor performance is due to the absence of REASONING in the base model, which is crucial to solve math problems. We can see the switch solving the same tasks with the REASONING model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "37eaab64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ qwen3_reasoning\\qwen3-0.6B-reasoning.pth already up-to-date\n",
      "✓ qwen3_reasoning\\tokenizer-reasoning.json already up-to-date\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Utente\\AppData\\Local\\Temp\\ipykernel_719604\\2595948078.py:23: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(model_path))\n"
     ]
    }
   ],
   "source": [
    "model_reasoning, tokenizer_reasoning = load_model_and_tokenizer(\n",
    "    which_model='reasoning',\n",
    "    device=device,\n",
    "    use_compile=False, local_dir='qwen3_reasoning'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "e5adb332",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: reasoning\n",
      "Device: cuda\n",
      "MATH-500: 10/10 | ETA: 00s25s\n",
      "Accuracy: 90.0% (9/10)\n",
      "Total time: 5.1 min\n",
      "Logs written to: math500-cuda-reasoning.jsonl\n"
     ]
    }
   ],
   "source": [
    "print(\"Model:\", 'reasoning')\n",
    "print(f'Device: {device}')\n",
    "num_correct, num_examples, acc = evaluate_math500_stream(\n",
    "    model_reasoning, 'reasoning', tokenizer_reasoning, device,\n",
    "    math_data=math_data[:10], #only evaluates the first 10 examples\n",
    "    max_new_tokens=2048,\n",
    "    verbose=False \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee2a4447",
   "metadata": {},
   "source": [
    "The reasoning model performs way better (90% vs 20%) but is more compute heavy, as deomnstarted by the execution time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "6b01499f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_average_response_length(report_path, tokenizer):\n",
    "    if not report_path:\n",
    "        return None\n",
    "    \n",
    "    path = Path(report_path)\n",
    "    tot_len = 0\n",
    "    tot_els = 0\n",
    "\n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        for row in f:\n",
    "            json_row = json.loads(row) #converts a string, while load() a file\n",
    "            generated_text = tokenizer.encode(json_row['generated_text'])\n",
    "            tot_len += len(generated_text)\n",
    "            tot_els += 1\n",
    "    \n",
    "    return float(tot_len/(1.0*tot_els))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "af50ec52",
   "metadata": {},
   "outputs": [],
   "source": [
    "reasoning_tokenizer_path = Path('qwen3_reasoning') / 'tokenizer-reasoning.json' \n",
    "base_tokenizer_path = Path('qwen3') / 'tokenizer-base.json' \n",
    "\n",
    "reasoning_tokenizer = Qwen3Tokenizer(reasoning_tokenizer_path, apply_chat_template=True)\n",
    "base_tokenizer = Qwen3Tokenizer(base_tokenizer_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "f3aac1ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average length of BASE model response: 6.8 tokens\n",
      "Average length of REASONING model response: 891.4 tokens\n"
     ]
    }
   ],
   "source": [
    "print(f'Average length of BASE model response: {compute_average_response_length('math500-cuda-base.jsonl', tokenizer=base_tokenizer)} tokens')\n",
    "print(f'Average length of REASONING model response: {compute_average_response_length('math500-cuda-reasoning.jsonl', tokenizer=reasoning_tokenizer)} tokens')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed586e03",
   "metadata": {},
   "source": [
    "We see a huge difference!, now let's try with the chat template and the keyword 'Problem' instead of 'Question'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "b12698f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def render_prompt(prompt):\n",
    "    template = \"\"\" \n",
    "You are an helpfu and powerful assistant.\n",
    "Answer the question and write the final result on a new line as:\n",
    "\\\\boxed{{ANSWER}}\n",
    "\n",
    "Problem:\n",
    "{PROMPT}\n",
    "\n",
    "Answer:\n",
    "    \"\"\".format(PROMPT=prompt)\n",
    "\n",
    "    return template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "8c432d15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: base\n",
      "Device: cuda\n",
      "MATH-500: 10/10 | ETA: 00s\n",
      "Accuracy: 20.0% (2/10)\n",
      "Total time: 0.1 min\n",
      "Logs written to: math500-cuda-base.jsonl\n"
     ]
    }
   ],
   "source": [
    "print(\"Model:\", WHICH_MODEL)\n",
    "print(f'Device: {device}')\n",
    "num_correct, num_examples, acc = evaluate_math500_stream(\n",
    "    model, WHICH_MODEL, base_tokenizer, device,\n",
    "    math_data=math_data[:10], #only evaluates the first 10 examples\n",
    "    max_new_tokens=2048,\n",
    "    verbose=False \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e94904d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average length of BASE model response: 10.1 tokens\n"
     ]
    }
   ],
   "source": [
    "print(f'Average length of BASE model response: {compute_average_response_length('math500-cuda-base.jsonl', tokenizer=base_tokenizer)} tokens')\n",
    " #more tokens generated (same performance on this small dataset, but overall would increase from 15% to 31%)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
